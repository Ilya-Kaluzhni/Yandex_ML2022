{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/main/Seminars/lab_02_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BRCKrupOesLn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz1LUfwtesLs"
      },
      "source": [
        "# Today's data\n",
        "\n",
        "400 fotos of human faces. Each face is a 2d array [64x64] of pixel brightness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "melNrhQoesLt"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_olivetti_faces\n",
        "data = fetch_olivetti_faces().images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA2Q-6P2esLy"
      },
      "outputs": [],
      "source": [
        "# this code showcases matplotlib subplots. The syntax is: plt.subplot(height, width, index_starting_from_1)\n",
        "plt.subplot(2,2,1)\n",
        "plt.imshow(data[0],cmap='gray')\n",
        "plt.subplot(2,2,2)\n",
        "plt.imshow(data[1],cmap='gray')\n",
        "plt.subplot(2,2,3)\n",
        "plt.imshow(data[2],cmap='gray')\n",
        "plt.subplot(2,2,4)\n",
        "plt.imshow(data[3],cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hbCClhYesL5"
      },
      "source": [
        "# Face reconstruction problem\n",
        "\n",
        "Let's solve the face reconstruction problem: given left halves of facex __(X)__, our algorithm shall predict the right half __(y)__. Our first step is to slice the photos into X and y using slices.\n",
        "\n",
        "__Slices in numpy:__\n",
        "* In regular python, slice looks roughly like this: `a[2:5]` _(select elements from 2 to 5)_\n",
        "* Numpy allows you to slice N-dimensional arrays along each dimension: [image_index, height, width]\n",
        "  * `data[:10]` - Select first 10 images\n",
        "  * `data[:, :10]` - For all images, select a horizontal stripe 10 pixels high at the top of the image\n",
        "  * `data[10:20, :, -25:-15]` - Take images [10, 11, ..., 19], for each image select a _vetrical stripe_ of width 10 pixels, 15 pixels away from the _right_ side.\n",
        "\n",
        "__Your task:__\n",
        "\n",
        "Let's use slices to select all __left image halves as X__ and all __right halves as y__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eV3HsLhQesL6"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUUu5T-uesL-"
      },
      "outputs": [],
      "source": [
        "# select left half of each face as X, right half as Y\n",
        "X = <Slice left half-images>\n",
        "y = <Slice right half-images>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cW4F11b_esMD"
      },
      "outputs": [],
      "source": [
        "# If you did everything right, you're gonna see left half-image and right half-image drawn separately in natural order\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(X[0],cmap='gray')\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(y[0],cmap='gray')\n",
        "\n",
        "assert X.shape == y.shape == (len(data), 64, 32), \"Please slice exactly the left half-face to X and right half-face to Y\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2Rxit8SesMH"
      },
      "outputs": [],
      "source": [
        "def glue(left_half,right_half):\n",
        "    # merge photos back together\n",
        "    left_half = left_half.reshape([-1, 64, 32])\n",
        "    right_half = right_half.reshape([-1, 64, 32])\n",
        "    return np.concatenate([left_half, right_half], axis=-1)\n",
        "\n",
        "\n",
        "# if you did everything right, you're gonna see a valid face\n",
        "plt.imshow(glue(X, y)[99], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcvCMAogesML"
      },
      "source": [
        "# Linear regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_ajG2zoesMM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X.reshape([len(X), -1]),\n",
        "                                                    y.reshape([len(y), -1]),\n",
        "                                                    test_size=0.05, random_state=42)\n",
        "\n",
        "print(X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3timNAgJesMR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPXoDAiqesMW"
      },
      "source": [
        "measure mean squared error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH2HuGi4esMY"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print(\"Train MSE:\", mean_squared_error(Y_train, model.predict(X_train)))\n",
        "print(\"Test MSE:\", mean_squared_error(Y_test, model.predict(X_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-e00MeoesMb"
      },
      "source": [
        "## Why train error is much smaller than test?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj0_7ZZFgAya"
      },
      "outputs": [],
      "source": [
        "# Train predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and predicted Y for the train dataset\n",
        "\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MCiLbOTugSV8"
      },
      "outputs": [],
      "source": [
        "# Test predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and predicted Y for the test dataset\n",
        "\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZZF6Ns0gsIm"
      },
      "source": [
        "### Regularisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPtPdBCMesMm"
      },
      "source": [
        "There are  many linear models in sklearn package, and all of them can be found [here](https://scikit-learn.org/stable/modules/linear_model.html). We will focus on 3 of them: Ridge regression, Lasso and ElasticNet.\n",
        "Idea of all of them is very simple: add some penalty to the objective loss function to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucmL5LNSesMm"
      },
      "source": [
        "# Ridge regression\n",
        "RidgeRegression is just a LinearRegression, with l2 regularization - penalized for $ \\alpha \\cdot \\sum _i w_i^2$\n",
        "\n",
        "Let's train such a model with alpha=0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IxtafdhesMo"
      },
      "outputs": [],
      "source": [
        "from <YOUR CODE> import <YOUR CODE>\n",
        "\n",
        "ridge = <YOUR CODE>\n",
        "\n",
        "<YOUR CODE: fit the model on training set>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0UAnpxQesMr"
      },
      "outputs": [],
      "source": [
        "<YOUR CODE: predict and measure MSE on train and test>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgOK0gfFiirK"
      },
      "outputs": [],
      "source": [
        "# Test predictions\n",
        "pics = <YOUR CODE> # reconstruct and glue together X and predicted Y for the test dataset\n",
        "\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0xpbcZVesMy"
      },
      "source": [
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# Finding the best `alpha` (grid search)\n",
        "\n",
        "`sklearn` has a pre-implemented class - `sklearn.model_selection.GridSearchCV` - that wraps your model and optimizes its hyperparameters using K-fold cross-validation. The hyperparameter values are taken from a finite set of values in a rectangular grid (therefore, the method is called grid search). In order to use it, you need to set the hyperparameter values grid, the metric you want to optimize and the number of folds from the cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qgy_LgwNesMz"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8ETgdXIesM1"
      },
      "outputs": [],
      "source": [
        "parameter_dict = {\n",
        "    \"alpha\" : np.logspace(-3, 3, 13, base=10)\n",
        "}\n",
        "\n",
        "gscv = GridSearchCV(\n",
        "    estimator=Ridge(), # our model to optimize\n",
        "    param_grid=parameter_dict, # grid of parameter values\n",
        "    scoring='neg_mean_squared_error', # metric - it needs to be a score, so\n",
        "                                      # we take the negative MSE\n",
        "    cv=5, verbose=2, n_jobs=-1\n",
        ")\n",
        "gscv.fit(X_train, Y_train)\n",
        "\n",
        "plt.errorbar(gscv.param_grid['alpha'],\n",
        "             gscv.cv_results_['mean_test_score'],\n",
        "             gscv.cv_results_['std_test_score'] / gscv.cv**0.5,\n",
        "             capsize=5)\n",
        "plt.xscale(\"log\", nonposx='clip')\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"negative MSE\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBeswJPABJpQ"
      },
      "source": [
        "Now you can find the best model as `gscv.best_estimator_`. Use it to make the reconstruction on the test again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQGIIuhbesNA"
      },
      "outputs": [],
      "source": [
        "# Test predictions\n",
        "pics = glue(X_test, <predict with your best model>)\n",
        "plt.figure(figsize=[16, 12])\n",
        "for i in range(20):\n",
        "    plt.subplot(4, 5, i + 1)\n",
        "    plt.imshow(pics[i], cmap='gray')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "433Mv13AesNG"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Lasso, ElasticNet\n",
        "\n",
        "# Use the code above to do GridSearch for Lasso and/or ElasticNet\n",
        "# models. Note that Lasso and ElasticNet are *much*\n",
        "# slower to fit, compared to Ridge (especially for small alpha).\n",
        "<YOUR CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVe4ROnLFAg_"
      },
      "source": [
        "## Add transformers and make a pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9czWvlFFFmN"
      },
      "source": [
        "```python\n",
        "some_transformer = sklearn.some_module.SomeTransformerClass(some_parameters) # create the transformer object\n",
        "some_transformer.fit(X, y) # learn how to transform the data (e.g.\n",
        "                           # for StandardScaler, calculate mean and std of columns in X)\n",
        "some_transformer.transform(X) # transform the features\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nLC9az4FDgE"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#make a new Ridge model using a pipeline with StandardScaler() and  Ridge regression\n",
        "#fit the model and print 'Train MSE' and 'Test MSE'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4Ww237DKTQX"
      },
      "source": [
        "This pipeline can be used within grid search CV, e.g.:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mpu7hE-GKLy3"
      },
      "outputs": [],
      "source": [
        "parameter_dict = {\n",
        "    \"ridge__alpha\" : np.logspace(-1, 7, 9, base=10) # note the 'ridge__' prefix\n",
        "                                                    # that tells to which step\n",
        "                                                    # of the pipeline this\n",
        "                                                    # parameter belongs\n",
        "}\n",
        "\n",
        "gscv = GridSearchCV(\n",
        "    estimator=model,\n",
        "    param_grid=parameter_dict,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=5, verbose=2, n_jobs=-1\n",
        ")\n",
        "gscv.fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "plt.errorbar(gscv.param_grid['ridge__alpha'],\n",
        "             gscv.cv_results_['mean_test_score'],\n",
        "             gscv.cv_results_['std_test_score'] / gscv.cv**0.5,\n",
        "             capsize=5)\n",
        "plt.xscale(\"log\", nonposx='clip')\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"negative MSE\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNncsNN4IWh9"
      },
      "source": [
        "See also:\n",
        "- [sklearn.preprocessing.MinMaxScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) - scaling based on min and max values of the feature\n",
        "- [sklearn.preprocessing.RobustScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html) - scaling that ignores the tails of the feature distribution (more robust to outliers)\n",
        "- [sklearn.preprocessing.OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) - one-hot encoding for categorical features\n",
        "- [sklearn.preprocessing.FunctionTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html) - transformation using arbitrary (e.g. user defined) python function\n",
        "- [sklearn.compose.ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) - tool that allows composing complicated nested structures of transformers (e.g. for using different transformers for different features)\n",
        "-[sklearn.compose.make_column_selector](https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html) - tool for selection columns with specified data type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK8MqkJ4esNJ"
      },
      "source": [
        "# Outliers impact on regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN4QiVwKesNK"
      },
      "source": [
        "Remember, that when we minimise loss\n",
        "$$\n",
        "MSE = (\\hat y - y)^2\n",
        "$$\n",
        "\n",
        "which penalise more to for heigher values of error. What is that impact of this for the dataset with outliers, what do you think?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FP4DRSJ_esNL"
      },
      "source": [
        "Here is an example of regression fitted with ordinary LR and RANSAC, which iteratively trains on random subsamples of the data, trying to identify outliers.\n",
        "\n",
        "`class sklearn.linear_model.RANSACRegressor(base_estimator=None, *, min_samples=None, residual_threshold=None, is_data_valid=None, is_model_valid=None, max_trials=100, max_skips=inf, stop_n_inliers=inf, stop_score=inf, stop_probability=0.99, loss='absolute_error', random_state=None)`\n",
        "\n",
        "RANSAC regressor algorithm:\n",
        "- Select min_samples random samples from the original data and check whether the set of data is valid (see is_data_valid).\n",
        "\n",
        "- Fit a model to the random subset (base_estimator.fit) and check whether the estimated model is valid (see is_model_valid).\n",
        "\n",
        "- Classify all data as inliers or outliers by calculating the residuals to the estimated model (base_estimator.predict(X) - y) - all data samples with absolute residuals smaller than or equal to the residual_threshold are considered as inliers.\n",
        "\n",
        "- Save fitted model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has better score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwDJZxuBesNM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from sklearn import linear_model, datasets\n",
        "\n",
        "\n",
        "n_samples = 1000\n",
        "n_outliers = 50\n",
        "\n",
        "\n",
        "X, y, coef = datasets.make_regression(n_samples=n_samples, n_features=1,\n",
        "                                      n_informative=1, noise=10,\n",
        "                                      coef=True, random_state=0)\n",
        "\n",
        "# Add outlier data\n",
        "np.random.seed(0)\n",
        "X[:n_outliers] = 3 + 0.5 * np.random.normal(size=(n_outliers, 1))\n",
        "y[:n_outliers] = -3 + 10 * np.random.normal(size=n_outliers)\n",
        "\n",
        "# Fit line using all data\n",
        "lr = linear_model.LinearRegression()\n",
        "lr.fit(X, y)\n",
        "\n",
        "# Robustly fit linear model with RANSAC algorithm\n",
        "ransac = linear_model.RANSACRegressor()\n",
        "ransac.fit(X, y)\n",
        "inlier_mask = ransac.inlier_mask_\n",
        "outlier_mask = np.logical_not(inlier_mask)\n",
        "\n",
        "# Predict data of estimated models\n",
        "line_X = np.arange(X.min(), X.max())[:, np.newaxis]\n",
        "line_y = lr.predict(line_X)\n",
        "line_y_ransac = ransac.predict(line_X)\n",
        "\n",
        "# Compare estimated coefficients\n",
        "print(\"Estimated coefficients (true, linear regression, RANSAC):\")\n",
        "print(coef, lr.coef_, ransac.estimator_.coef_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISdoPPi3esNQ",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "lw = 2\n",
        "plt.scatter(X[inlier_mask], y[inlier_mask], color='yellowgreen', marker='.',\n",
        "            label='Inliers')\n",
        "plt.scatter(X[outlier_mask], y[outlier_mask], color='gold', marker='.',\n",
        "            label='Outliers')\n",
        "plt.plot(line_X, line_y, color='navy', linewidth=lw, label='Linear regressor')\n",
        "plt.plot(line_X, line_y_ransac, color='cornflowerblue', linewidth=lw,\n",
        "         label='RANSAC regressor')\n",
        "plt.legend(loc='lower right')\n",
        "plt.xlabel(\"Input\")\n",
        "plt.ylabel(\"Response\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJkJpSK4qNzl"
      },
      "source": [
        "## Bonus part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZBAqmrrqQch"
      },
      "source": [
        "Try using `sklearn.linear_model.SGDRegressor` with `huber` loss in the code above instead of `LinearRegression`. Is it better in this case? Try varying its `epsilon` parameter."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lab2_1_regression_seminar.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}