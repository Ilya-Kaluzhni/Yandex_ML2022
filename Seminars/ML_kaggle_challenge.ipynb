{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2021/blob/master/02_lab/kaggle_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVS9Ysjf_3Om"
      },
      "source": [
        "# Set up variables and download data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TegjGrHdBbvu"
      },
      "source": [
        "Register on [kaggle](https://www.kaggle.com) and accept the [competition](https://www.kaggle.com/c/mlimperial2022-predict-the-house-price/overview) rules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tk_xLxkyB3fh"
      },
      "source": [
        "Go to My Account and under API section click **create new API Token**.\n",
        "Download created kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xem4gLJRCG1o"
      },
      "source": [
        "Upload this file to your google drive root folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOCBykoqCTCW"
      },
      "source": [
        "Now execute the following magic. - It installs kaggle, mounts google drive and downloads data from competition to you drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNVBlzYo74g-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9it6I6mJ8D0N"
      },
      "outputs": [],
      "source": [
        "!mkdir /root/.kaggle\n",
        "!cp /content/gdrive/My\\ Drive/kaggle.json /root/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!ls -l /root/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1PUcjpmFJAs"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOs-l71b8EgR"
      },
      "outputs": [],
      "source": [
        "#!kaggle config set -n path -v /content\n",
        "!kaggle competitions download -c mlimperial2022-predict-the-house-price -p '/content/gdrive/My Drive/mlimperial2022-predict-the-house-price'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95rwAUYeG3da"
      },
      "outputs": [],
      "source": [
        "!unzip -q /content/gdrive/My\\ Drive/mlimperial2022-predict-the-house-price/mlimperial2022-predict-the-house-price.zip -d /content/gdrive/My\\ Drive/mlimperial2022-predict-the-house-price/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qpk_2c3GOtE"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"/content/gdrive/My Drive/mlimperial2022-predict-the-house-price\"\n",
        "!ls /content/gdrive/My\\ Drive/mlimperial2022-predict-the-house-price"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a8HaEMe_Ffs"
      },
      "source": [
        "# https://www.kaggle.com/c/mlimperial2022-predict-the-house-price/overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4twUlePA-tJ"
      },
      "source": [
        "### Metric\n",
        "\n",
        "For regression task we can use the most common Mean Squared Error(MSE). However, sometimes its better to use logarithmic error. In this challenge, we will use RMSLE - root mean square logarithmic error:\n",
        "\n",
        "$$\n",
        "RMSLE = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} [\\log(y_i + 1) - \\log(p_i + 1)]^2},\n",
        "$$\n",
        "\n",
        "where $y_i$ is true value and $p_i$ is a predicted value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z-MTx2FB3c1"
      },
      "source": [
        "# Grading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqB6ygyQDSaz"
      },
      "source": [
        "Your task is to try as many techniques that you have learned this week as possible.\n",
        "\n",
        "\n",
        "The outcome of your work should be a properly documented jupyter notebook, that contains all the experiments you did + your explanations/comments on them.\n",
        "\n",
        "\n",
        "The archive with jupyter notebook should be sent to mlicl-2022-seminars@mail.ru\n",
        " with the topic: Surname_name_kaggle_1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_TF_I3071k_"
      },
      "source": [
        "### The total amount of points is 10. You will get additional points based on your final ranking\n",
        "\n",
        "Start with baseline solution for convinience.\n",
        "\n",
        "- 1 Point. Work with missed values. Which features are better to remove? Are there any features worth fixing (filling values)?\n",
        "- 1 Point. Work with categorial features. Try to encode them (one-hot encoding, frequency encoding). Does this improve your score?\n",
        "- 1 Point. Work with the timestamps. What information can you extract from them? <i>Example</i>: convert them to separate year, month, day features.\n",
        "- 1 Point. Find highly correlated features in the train.csv and macro.csv (determine your own threshold). How does removing one of them affects the model prediction capability? Analyse the correlation between features and target. Decide what to do with features that have negative correlation with target (throw them away or process them)?\n",
        "- 1 Point. Compare various linear regression methods (Ridge, Lasso, SVR, SGD-based) and decision trees based algorithms (random forest, boosting). Grid search parameters.\n",
        "- 1 Point. Try to find badly defined features and outliers in the dataset. Remove them. Did it help?\n",
        "- 1 Point. Try generating your own features that produce improvement (ratio of life_sq to full_sq, age of a building, ratio of floor to max floor, ...)\n",
        "- 1 Point. Try using PCA on some subset of features (determine this subset). Is it usefull? Why? Visualise features with the highest explained variance.\n",
        "- 1 Point. Estimate feature importances w.r.t your best model. Try to remove the least important features. Which difference did you notice in comparison when you removed correlated features? Visualize dependency between target and two most important features.\n",
        "- 1 Point. Make stacking of the models trained above? Does it improve your score?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20He9w3471lA"
      },
      "source": [
        "## Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hg4sDAMr71lB"
      },
      "source": [
        "Beat medium baseline and we will give you +3 points :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_3xbCC_CQlT"
      },
      "source": [
        "# Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG_2DJplCTah"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzyH18tHCWb7"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv(os.path.join(DATA_PATH, 'X_train.csv'), index_col=0, parse_dates=['timestamp'])\n",
        "X_test = pd.read_csv(os.path.join(DATA_PATH, 'X_test.csv'), index_col=0, parse_dates=['timestamp'])\n",
        "y_train = pd.read_csv(os.path.join(DATA_PATH, 'y_train.csv'), index_col=0)\n",
        "\n",
        "macro = pd.read_csv(os.path.join(DATA_PATH, \"macro.csv\"), index_col=0, parse_dates=['timestamp'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRHy9F5roTJ9"
      },
      "source": [
        "### Explore train data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4JjdJ1toTJ9"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4r1MzeIoTJ-"
      },
      "outputs": [],
      "source": [
        "X_train.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfBaSW-koTJ-"
      },
      "outputs": [],
      "source": [
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFj7MQdloTJ-"
      },
      "outputs": [],
      "source": [
        "y_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FE782vBoTJ-"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
        "\n",
        "sns.histplot(y_train['price_doc'], ax=ax[0]);\n",
        "ax[0].grid();ax[0].set_title('Price doc distr')\n",
        "sns.histplot(np.log1p(y_train['price_doc']), ax=ax[1]);\n",
        "ax[1].grid();ax[1].set_title('log Price doc distr');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9F71vrXoTJ_"
      },
      "source": [
        "### Let's see what 'macro' data offers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsrAkCi_oTJ_"
      },
      "outputs": [],
      "source": [
        "macro.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArkktrP1DH0P"
      },
      "source": [
        "As you can see, timestamps are important here, because it will define the various variables, that change in time. For example, we could merge two data tables (X_train and macro) by timestamps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-5k4fRrD6Wl",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "X_train_aug = pd.merge(X_train, macro, on='timestamp', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaVTKO2GFNWy"
      },
      "outputs": [],
      "source": [
        "X_train_aug.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcLPzlCNFWnJ"
      },
      "source": [
        "Before that we proceed to model training, we must get rid of NaNs and non-numerical data!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYrdvg7UoTKA"
      },
      "source": [
        "### Working with missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qw6H7UkLoTKA"
      },
      "outputs": [],
      "source": [
        "# calculate ratio of missing values to total size of a column\n",
        "nan_ratio = X_train_aug.isna().sum(axis=0)/len(X_train_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCvJQFh5oTKA"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(np.arange(len(nan_ratio)), nan_ratio);plt.grid();plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mvtcqz4koTKB"
      },
      "outputs": [],
      "source": [
        "def process_nans(X):\n",
        "    # select numerical columns\n",
        "    numeric_cols = X.columns[(X.dtypes == 'int64') | (X.dtypes == 'float64')]\n",
        "    \n",
        "    # replace all NaN with mean value of corresponding column\n",
        "    X[numeric_cols] = X[numeric_cols].fillna(X[numeric_cols].mean()).copy()\n",
        "    return X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSvLVJ1uFfZC"
      },
      "outputs": [],
      "source": [
        "X_train_aug = process_nans(X_train_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IheELZmoTKB"
      },
      "outputs": [],
      "source": [
        "X_train_aug.isna().sum(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXjpT8o6oTKB"
      },
      "source": [
        "Have all NaNs gone?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JT7sQwcoTKB"
      },
      "source": [
        "### Working with object data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KphMU77NoTKC"
      },
      "outputs": [],
      "source": [
        "# select all 'string' columns (called Object type)\n",
        "obj_cols = X_train_aug.columns[X_train_aug.dtypes == 'O']\n",
        "X_train_aug[obj_cols].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxTGh6zPoTKC"
      },
      "source": [
        "For now we will just drop columns of 'Object' type within the baseline solution. In YOUR solution, of course, you should find a way to extract vital information out of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS7FggbCoTKC"
      },
      "outputs": [],
      "source": [
        "def process_obj(X):\n",
        "    # just drop them for now\n",
        "    obj_cols = X.columns[X.dtypes == 'O']\n",
        "    return X.drop(columns = obj_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ej_6kuydoTKD"
      },
      "outputs": [],
      "source": [
        "X_train_aug = process_obj(X_train_aug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETd5yeM5oTKD"
      },
      "source": [
        "### Working with timestamp data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpMTb3BKoTKD"
      },
      "source": [
        "Same goes with 'datetime' data. We just drop it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-zpC2vXoTKD"
      },
      "outputs": [],
      "source": [
        "def process_ts(X):\n",
        "    ts_cols = X.columns[X.dtypes == \"datetime64[ns]\"]\n",
        "    return X.drop(columns = ts_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzpCnq6MoTKE"
      },
      "outputs": [],
      "source": [
        "X_train_aug = process_ts(X_train_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Se5Wrec5oTKE"
      },
      "outputs": [],
      "source": [
        "X_train_aug.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPzr6OGioTKE"
      },
      "outputs": [],
      "source": [
        "X_train_aug.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euYSveQNoTKE"
      },
      "source": [
        "### Feature filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy28FiNEoTKE"
      },
      "source": [
        "Feature filtering is dedicated to feature postprocessing: dropping unnecessary features, transforming them or generating new ones (by hand, PCA, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5oJBzaNoTKF"
      },
      "outputs": [],
      "source": [
        "def filter_feats(X):\n",
        "    return X.drop(columns = ['id'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afvsjxaxoTKF"
      },
      "outputs": [],
      "source": [
        "X_train_aug = filter_feats(X_train_aug)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93LqJTF0oTKF"
      },
      "source": [
        "### Scaling data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6PfDDR9oTKG"
      },
      "source": [
        "Make sure our features are normalised:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-L2i7tVoTKF"
      },
      "source": [
        "Scale data before passing into algorithms. NOTE that outliers affect some of scaling techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8BIIy1RoTKF"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def scale_data(X):\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "    return X_scaled, scaler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = X_train_aug.columns"
      ],
      "metadata": {
        "id": "Ep6GIJLkzkTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGYSl-GWoTKG"
      },
      "outputs": [],
      "source": [
        "X_train_aug, scaler = scale_data(X_train_aug)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlY14JRyoTKG"
      },
      "outputs": [],
      "source": [
        "X_train_aug.mean()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7hlwha6oTKG"
      },
      "outputs": [],
      "source": [
        "X_train_aug.std(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Socz_QO0oTKH"
      },
      "source": [
        "### Forming preprocess pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnyLOrDjoTKH"
      },
      "source": [
        "Let's create our pipeline procedure that incorporates all operations/transformations with data we did before. We would need that in the future in order to apply our transformations to X_test (the SAME way as for X_train!)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fxCE1sJoTKH"
      },
      "outputs": [],
      "source": [
        "def my_preprocessing(X, scaler=None):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "    - pandas table X\n",
        "    - sklearn Scaler (None for train, not None for test)\n",
        "    \n",
        "    Applies the following transformations to input X (within baseline):\n",
        "    - Merge X with macro data by timestamps\n",
        "    - Replace all NaNs for numerical features with their column mean value\n",
        "    - Drop all Object-type data (strings)\n",
        "    - Drop all timestamps\n",
        "    - Drop id column\n",
        "    - Scale features (converts pandas table to numpy array)\n",
        "    \n",
        "    Output:\n",
        "    - Transformed X_preproc\n",
        "    - sklearn Scaler that we use later for X_test scaling\n",
        "    \"\"\"\n",
        "    # augment data with macro data\n",
        "    X_preproc = pd.merge(X, macro, on='timestamp', how='left')\n",
        "    \n",
        "    # remove or fix NaNs\n",
        "    X_preproc = process_nans(X_preproc)\n",
        "    \n",
        "    # Encode object values (of type 'string') into integer values\n",
        "    X_preproc = process_obj(X_preproc)\n",
        "    \n",
        "    # Encode timestamps (into) into integer values\n",
        "    X_preproc = process_ts(X_preproc)\n",
        "    \n",
        "    # Do feature filtering ! MOST IMPORTANT part\n",
        "    X_preproc = filter_feats(X_preproc)\n",
        "    \n",
        "    col_names = X_preproc.columns\n",
        "    \n",
        "    # Apply scaler if avaiable (test) or create and apply one (train)\n",
        "    if (scaler is None):\n",
        "        X_preproc, scaler = scale_data(X_preproc)\n",
        "    else:\n",
        "        X_preproc = scaler.transform(X_preproc)\n",
        "        \n",
        "    return X_preproc, scaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svvJqc_1oTKI"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rt8WuLIZoTKI"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "# Create our metric (Loss)\n",
        "def RMSLE(log_y_pred, log_y_true):\n",
        "    return mean_squared_error(log_y_pred, log_y_true, squared=False)\n",
        "\n",
        "scorer = make_scorer(RMSLE, greater_is_better=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maiPE4o8oTKI"
      },
      "source": [
        "Let's train basic linear Ridge regression on whole train dataset. Is that a good idea? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgXgg25FHbnA"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "# Train on whole training data\n",
        "predictor = Ridge(alpha=1.0)\n",
        "predictor.fit(X_train_aug, np.log1p(y_train['price_doc']))\n",
        "\n",
        "RMSLE(predictor.predict(X_train_aug), np.log1p(y_train['price_doc']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_stNePaoTKI"
      },
      "source": [
        "What was our model trained to?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av7Lux1KoTKI"
      },
      "source": [
        "Now let's run cross-validation procedure. Before that we need to specify the number of folds (parameter cv) and grid of parameters (alpha)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWnHb-NIHDMB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10]}\n",
        "\n",
        "gscv = GridSearchCV(predictor, param_grid, scoring=scorer, cv=5, n_jobs=-1, verbose=1)\n",
        "gscv.fit(X_train_aug, np.log1p(y_train['price_doc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYOvyRFyoTKJ"
      },
      "outputs": [],
      "source": [
        "for ind, alpha in enumerate(param_grid['alpha']):\n",
        "    print(f'alpha = {alpha} || Mean cv-loss = {-gscv.cv_results_[\"mean_test_score\"][ind]:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWsrhDhToTKJ"
      },
      "source": [
        "Why do we multiply out mean test score by '-1' ? We can see that with alpha=1.0 cv-loss differs from training loss, why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rePz4cTqoTKJ"
      },
      "source": [
        "Find best 'alpha':"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKbsPBJ1HFtP",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "best_score_ind = np.argmax(gscv.cv_results_['mean_test_score'])\n",
        "gscv.cv_results_['params'][best_score_ind], -gscv.cv_results_['mean_test_score'][best_score_ind]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxhCLtuqoTKK"
      },
      "outputs": [],
      "source": [
        "predictor = Ridge(alpha=10.0)\n",
        "predictor.fit(X_train_aug, np.log1p(y_train['price_doc']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sy0dmXsHoTKK"
      },
      "outputs": [],
      "source": [
        "col_names[np.argsort(predictor.coef_)[-20:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsswrvuHRh7N"
      },
      "source": [
        "# Make predictions on the test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgwkAIC1zeaM"
      },
      "outputs": [],
      "source": [
        "X_test_preproc, _ = my_preprocessing(X_test, scaler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMRVrpd4RRa2"
      },
      "outputs": [],
      "source": [
        "# inverse of y_log = log(1 + y) -> y = exp(y_log) - 1\n",
        "prediction = np.expm1(predictor.predict(X_test_preproc))\n",
        "prediction = pd.DataFrame(np.stack((X_test['id'], prediction), axis=1), columns=['id', \"price_doc\"])\n",
        "prediction['id'] = prediction['id'].astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccA7_auvS8lh"
      },
      "outputs": [],
      "source": [
        "prediction.to_csv(os.path.join(DATA_PATH, \"prediction.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wOSSUbBV2Ka"
      },
      "outputs": [],
      "source": [
        "!head -n 5 '/content/gdrive/My Drive/mlimperial2022-predict-the-house-price/prediction.csv'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-62qk7aIWU2R"
      },
      "source": [
        "# Lets use kaggle API again to submit results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_2E9T8pWLlT"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions submit -c mlimperial2022-predict-the-house-price -f \"{DATA_PATH}/prediction.csv\" -m \"Test\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "kaggle_challenge.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}