{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/master/Seminars/lab_08_02_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "T1DFX2IEAuYW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xleq-DVssr-9"
      },
      "source": [
        "# Autoencoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfYL1T3Ksr--"
      },
      "source": [
        "# Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMP6i1Kgsr--"
      },
      "source": [
        "## Variational Autoencoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKvA0rZNsr-_"
      },
      "source": [
        "### Problem setting\n",
        "\n",
        "A set of independent and identically distributed samples from true data distribution is given: $x_i \\sim p_{true}(x)$, $i = 1, \\dots, N$.\n",
        "\n",
        "The problem is to build a probabilistic model $p_\\theta(x)$ of the true data distribution $p_{true}(x)$.\n",
        "\n",
        "The model $p_\\theta(x)$ must be able to estimate probabilistic density function (p. d. f.) for given $x$ and to sample $x \\sim p_\\theta(x)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fo00gzrXsr-_"
      },
      "source": [
        "### Probabilistic model\n",
        "$z \\in \\mathbb{R}^d$ is a latent variable.\n",
        "\n",
        "The generative process of VAE:\n",
        "1. Sample $z \\sim p(z)$.\n",
        "2. Sample $x \\sim p_\\theta(x | z)$.\n",
        "\n",
        "The parameters of distribution $p_\\theta(x | z)$ are obtained using a neural network with weights $\\theta$ and $z$ as an input.\n",
        "This network is called generator or decoder.\n",
        "\n",
        "The above generative process induce the following model p. d. f. for $x$:\n",
        "\n",
        "$$p_\\theta(x) = \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1dyzCf_sr_A"
      },
      "source": [
        "### Model parameterization\n",
        "\n",
        "A priori distribution on the latent variables is standard normal distribution: $p(z) = \\mathcal{N}(z | 0, I)$.\n",
        "\n",
        "The distributions on the components of $x$ are conditionally independent given $z$: $p_\\theta(x | z) = \\prod\\limits_{i = 1}^D p_\\theta(x_i | z)$.\n",
        "\n",
        "If i-th component is real-valued, we can use Gaussian generative distribution: $p_\\theta(x_i | z) = \\mathcal{N}(x_i | \\mu_i(z, \\theta), \\sigma^2_i(z, \\theta))$.\n",
        "Here $\\mu(z, \\theta)$ Ð¸ $\\sigma(z, \\theta)$ are deterministic functions defined by neural networks with parameters $\\theta$.\n",
        "\n",
        "If i-th component is categorial, then we can use categorical generative distribution: $p_\\theta(x_i | z) = Cat(Softmax(\\omega_i(z, \\theta)))$, where $\\omega_i(z, \\theta)$ is also a deterministic function described by neural network.\n",
        "\n",
        "Binary components are the special case of categorical ones. For them categorical distribution turns into Bernoulli distibution with just one parameter.\n",
        "\n",
        "_Tip:_ some pixels are black in the whole MNIST train set, so likelihood maximization forces the probability of these pixels to be black to 1.\n",
        "Therefore the weights for these pixels go to infinity.\n",
        "To avoid divergence of the training procedure, we may add a clipping level into generative network: e. g. clipping layer into range $[-10, 10]$ before final activation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfkhGJhYsr_A"
      },
      "source": [
        "### Variational lower bound\n",
        "\n",
        "To fit the model to data we maximize marginal log-likelihood $\\log p_\\theta(x)$ of the train set.\n",
        "\n",
        "Nevertheless, $\\log p_\\theta(x)$ cannot be optimized straightforwardly, because there is integral in high-dimensional space inside the logarithm which cannot be computed analytically or numerically estimated with enough accuracy in a reasonable amount of time.\n",
        "\n",
        "So to perform optimization we maximize the _variational lower bound_ (VLB) on log-likelihood instead:\n",
        "$$\\log p_\\theta(x) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x) = \n",
        "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z) q_\\phi(z | x)}{q_\\phi(z | x) p_\\theta(z | x)} = \n",
        "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x, z)}{q_\\phi(z | x)} + KL(q_\\phi(z | x) || p_\\theta(z | x))$$\n",
        "$$\\log p_\\theta(x) \\geqslant \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log \\frac{p_\\theta(x | z)p(z)}{q_\\phi(z | x)} = \n",
        "\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z) - KL(q_\\phi(z | x) || p(z)) = L(x; \\phi, \\theta)\n",
        "\\to \\max\\limits_{\\phi, \\theta}$$\n",
        "\n",
        "$q_\\phi(z | x)$ is called a proposal, recognition or variational distribution. It is usually defined as a Gaussian with parameters from a neural network with weights $\\phi$ which takes $x$ as an input:\n",
        "$q_\\phi(z | x) = \\mathcal{N}(z | \\mu_\\phi(x), \\sigma^2_\\phi(x)I)$.\n",
        "Usually neural network defines $\\log\\sigma_\\phi(x)$ or $\\log(\\exp(\\sigma_\\phi(x) - 1))$ instead of $\\sigma_\\phi(x)$. So $\\sigma_\\phi(x)$ is always positive by design and also more scale-independent.\n",
        "\n",
        "### Discussion of VLB\n",
        "\n",
        "One can show that the gap between VLB $L(x; \\phi, \\theta)$ on log-likelihood and the log-likelihood $\\log p_\\theta(x)$ itself is KL-divergence between proposal and aposteriori distributions over $z$: $KL(q_\\phi(z | x) || p_\\theta(z | x))$.\n",
        "Maximum of $L(x; \\phi, \\theta)$ with fixed $\\theta$ is achieved when $q_\\phi(z | x) = p_\\theta(z | x)$.\n",
        "Nevertheless, $p_\\theta(z | x)$ is untracable, so instead of numerically computing it, VLB is optimized w. r. t. $\\phi$ using backpropagation and reparameterization trick (see below).\n",
        "The closer $q_\\phi(z | x)$ to $p_\\theta(z | x)$, the more precise is VLB.\n",
        "The true posterior distribution $p_\\theta(z | x)$ often cannot be decribed by one Gaussian, so the gap between VLB and LL never reaches zero.\n",
        "\n",
        "The first term of VLB - $\\mathbb{E}_{z \\sim q_\\phi(z | x)} \\log p_\\theta(x | z)$ - is called reconstruction loss.\n",
        "The model describes this term is an autoencoder with one stochastic layer which tries to restore input object $x$.\n",
        "If $q_\\phi(z | x)$ is a delta-function, then an autoencoder with a stochastic layer turns into an ordinary autoencoder.\n",
        "That is why $q_\\phi(z | x)$ and $p_\\theta(x | z)$ are called encoder and decoder respectivelly.\n",
        "\n",
        "The term $KL(q_\\phi(z | x) || p(z))$ is called regularizer.\n",
        "It forces $z \\sim q_\\phi(z | x)$ to be close to $0$.\n",
        "But, as described above, it also forces $q_\\phi(z | x)$ to be close to $p_\\theta(z | x)$, which is even more important.\n",
        "One can use a coefficient before KL-divergence or even a different regularizer.\n",
        "Naturally, after that optimiation of VLB usually becomes unrelated to the log-likelihood of the initial probabilistic model.\n",
        "This decreases intrpretability of the model and avoids theoretical guarantees.\n",
        "\n",
        "KL-divergence between two Gaussians can be computed analytically, which improves the speed and stability of optimization procedure.\n",
        "\n",
        "### Reparameterization trick\n",
        "We use stochastic gradient ascent in order to maximize $L(x; \\phi, \\theta)$.\n",
        "\n",
        "The gradient of the reconstruction loss w. r. t. $\\theta$ is computed using backpropagation.\n",
        "$$\\frac{\\partial}{\\partial \\theta} L(x; \\phi, \\theta) = \\mathbb{E}_{z \\sim q_\\phi(z | x)} \\frac{\\partial}{\\partial \\theta} \\log p_\\theta(x | z)$$\n",
        "\n",
        "The gradient of the reconstruction loss w. r. t. $\\phi$ can be computed using reparametrization trick:\n",
        "$$\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)$$\n",
        "$$z = \\mu + \\sigma \\varepsilon \\Rightarrow z \\sim \\mathcal{N}(z | \\mu, \\sigma^2I)$$\n",
        "$$\\frac{\\partial}{\\partial \\phi} L(x; \\phi, \\theta) = \\mathbb{E}_{\\varepsilon \\sim \\mathcal{N}(\\varepsilon | 0, I)} \\frac{\\partial}{\\partial \\phi} \\log p_\\theta(x | \\mu_\\phi(x) + \\sigma_\\phi(x) \\varepsilon) - \\frac{\\partial}{\\partial \\phi} KL(q_\\phi(z | x) || p(z))$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMTMJFu_sr_B"
      },
      "source": [
        "### Log-likelihood estimation\n",
        "\n",
        "Model log-likelihood $\\log p_\\theta(x) = \\log \\mathbb{E}_{z \\sim p(z)} p_\\theta(x | z)$ is estimated using the hold-out validation set.\n",
        "\n",
        "Likelihood can be estimated using Monte-Carlo method:\n",
        "\n",
        "$$z_i \\sim p(z), i = 1, \\dots, K$$\n",
        "$$p_\\theta(x) \\approx \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i)$$\n",
        "\n",
        "This estimation above is unbiased, but also useless for us.\n",
        "\n",
        "For log-likelihood estimation the averaging is also performed inside the logarithm:\n",
        "$$\\log p_\\theta(x) \\approx \\log \\frac{1}{K} \\sum\\limits_{i = 1}^K p_\\theta(x | z_i),\\,\\,\\,\\,z_i \\sim p(z)$$\n",
        "\n",
        "Note that this estimate is biased now.\n",
        "\n",
        "\n",
        "![img](https://blog.bayeslabs.co/assets/img/vae-gaussian.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kllaQATasr_C"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8eySkAisr_C"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTDataset(Dataset):\n",
        "    def __init__(self, X, y=None, device='cuda'):\n",
        "        self.device = device\n",
        "        self.X, self.y = self.preprocess_data(X, y)\n",
        "        \n",
        "    def preprocess_data(self, X, y):\n",
        "        X_preproc = torch.tensor(X / 255.,\n",
        "                                    dtype=torch.float).reshape(-1, 28 * 28).to(self.device)\n",
        "        \n",
        "        if (y is None):\n",
        "            return X_preproc, None\n",
        "        \n",
        "        return X_preproc, torch.tensor(y).to(self.device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if (self.y is None):\n",
        "            return self.X[idx]\n",
        "        \n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "ZIqs6J-Es2Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Ub_BHwwnsr_D"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train = MNIST('mnist', download=True, train=True)\n",
        "train_ds = MNISTDataset(train.train_data, train.train_labels)\n",
        "train_dl = DataLoader(train_ds, batch_size = BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test = MNIST('mnist', download=True, train=False)\n",
        "test_ds = MNISTDataset(test.test_data, test.test_labels)\n",
        "test_dl = DataLoader(test_ds, batch_size = BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSx1ITFWsr_E"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def show_images(x):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    x = x.view(-1, 1, 28, 28).cpu()\n",
        "    mtx = torchvision.utils.make_grid(x, nrow=10, pad_value=1)\n",
        "    plt.imshow(mtx.permute([1, 2, 0]).numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "show_images(train_ds[:10][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGXqpM55sr_E"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self.train_loss_batch = []\n",
        "        self.train_loss_epoch = []\n",
        "\n",
        "        self.test_loss_batch = []\n",
        "        self.test_loss_epoch = []\n",
        "\n",
        "        self.test_LLMC_batch = []\n",
        "        self.test_LLMC_epoch = []\n",
        "\n",
        "        self.train_batches_per_epoch = 0\n",
        "        self.test_batches_per_epoch = 0\n",
        "        self.test_LLMC_batches_per_epoch = 0\n",
        "\n",
        "        self.epoch_counter = 0\n",
        "\n",
        "    def fill_train(self, loss):\n",
        "        self.train_loss_batch.append(loss)\n",
        "        self.train_batches_per_epoch += 1\n",
        "\n",
        "    def fill_test(self, loss):\n",
        "        self.test_loss_batch.append(loss)\n",
        "        self.test_batches_per_epoch += 1\n",
        "\n",
        "    def fill_test_LLMC(self, loss):\n",
        "        self.test_LLMC_batch.append(loss)\n",
        "        self.test_LLMC_batches_per_epoch += 1\n",
        "\n",
        "    def finish_epoch(self, make_plot=True):\n",
        "        self.train_loss_epoch.append(np.mean(\n",
        "            self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "        ))\n",
        "        self.test_loss_epoch.append(np.mean(\n",
        "            self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "        ))\n",
        "        self.test_LLMC_epoch.append(np.mean(\n",
        "            self.test_LLMC_batch[-self.test_LLMC_batches_per_epoch:]\n",
        "        ))\n",
        "        self.train_batches_per_epoch = 0\n",
        "        self.test_batches_per_epoch = 0\n",
        "        self.test_LLMC_batches_per_epoch = 0\n",
        "    \n",
        "        if make_plot:\n",
        "            clear_output()\n",
        "  \n",
        "        print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8} \\t LLMC: {:.8}\".format(\n",
        "                  self.epoch_counter,\n",
        "                  self.train_loss_epoch[-1],\n",
        "                  self.test_loss_epoch [-1],\n",
        "                  self.test_LLMC_epoch [-1]\n",
        "              ))\n",
        "    \n",
        "        self.epoch_counter += 1\n",
        "\n",
        "        if make_plot:\n",
        "            plt.figure(figsize=(11, 5))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(self.train_loss_batch, label='train loss')\n",
        "            plt.xlabel('# batch iteration')\n",
        "            plt.ylabel('loss')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "            plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "            plt.plot(self.test_LLMC_epoch , label='average test LLMC' )\n",
        "            plt.legend()\n",
        "            plt.xlabel('# epoch')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show();"
      ],
      "metadata": {
        "id": "ikSDQOHds1Iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, dl_train, dl_test, n_epochs, calc_LLMC=False, K=50):\n",
        "    logger = Logger()\n",
        "    \n",
        "    for i_epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for batch_X, _ in dl_train:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = model.calc_loss(batch_X)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            logger.fill_train(loss.item())\n",
        "            \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_X, _ in dl_test:\n",
        "                loss = model.calc_loss(batch_X)\n",
        "                if (calc_LLMC):\n",
        "                    LLMC = compute_log_likelihood_monte_carlo(batch_X, model, bernoulli_log_likelihood, K)\n",
        "                    logger.fill_test_LLMC(-LLMC)\n",
        "                logger.fill_test(loss.item())\n",
        "\n",
        "        logger.finish_epoch()"
      ],
      "metadata": {
        "id": "CeL7pj0Tuzmx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ClEW89ccsr_F"
      },
      "outputs": [],
      "source": [
        "n = 15\n",
        "digit_size = 28\n",
        "\n",
        "from scipy.stats import norm\n",
        "import numpy as np\n",
        "\n",
        "grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
        "\n",
        "def draw_manifold(generator):\n",
        "    figure = np.zeros((digit_size * n, digit_size * n))\n",
        "    for i, yi in enumerate(reversed(grid_x)):\n",
        "        for j, xi in enumerate(grid_y):\n",
        "            z_sample = np.array([[xi, yi]])\n",
        "\n",
        "            x_decoded = generator(z_sample)\n",
        "            digit = x_decoded\n",
        "            figure[i * digit_size: (i + 1) * digit_size,\n",
        "                   j * digit_size: (j + 1) * digit_size] = digit\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.imshow(figure, cmap='gray')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YG2jYfDsr_G"
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def draw_latent_space(model, data, target, use_TSNE=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(data).cpu().numpy()\n",
        "    if (use_TSNE):\n",
        "        z = TSNE(2).fit_transform(z)\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.scatter(z[:, 0], z[:, 1], c=target.cpu().numpy(), cmap='gist_rainbow', alpha=0.75)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrTM1Hhtsr_H"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehl5t2zgsr_H"
      },
      "outputs": [],
      "source": [
        "class AE(nn.Module):\n",
        "    def __init__(self, d, D):\n",
        "        \"\"\"\n",
        "        Initialize model weights.\n",
        "        Input: d, int - the dimensionality of the latent space.\n",
        "        Input: D, int - the dimensionality of the object space.\n",
        "        \"\"\"\n",
        "        super(type(self), self).__init__()\n",
        "        self.d = d\n",
        "        self.D = D\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(self.D, 200),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(200, self.d)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(self.d, 200),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Linear(200, self.D),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"\n",
        "        Generate a latent code given the objects.\n",
        "        Input: x, Tensor of shape n x D.\n",
        "        Return: Tensor of shape n x d.\n",
        "        \"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, z):\n",
        "        \"\"\"\n",
        "        Generate objects given the latent representations.\n",
        "        Input: z, Tensor of shape n x d - the latent representations.\n",
        "        Return: Tensor of shape n x D.\n",
        "        \"\"\"\n",
        "        return self.decoder(z)\n",
        "\n",
        "    def calc_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Compute batch loss. Batch loss is an average of per-object losses.\n",
        "        Per-object loss is a sum of reconstruction L2-error and\n",
        "        L2-regularization of the latent representations.\n",
        "\n",
        "        Input: batch, Tensor of shape n x D.\n",
        "        Return: Tensor, scalar - loss function for the batch.\n",
        "        \"\"\"\n",
        "        <YOUR_CODE>\n",
        "        return loss\n",
        "\n",
        "    def generate_samples(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate samples from standard normal distribution in the latent space.\n",
        "        Input: num_samples, int - number of sample to be generated.\n",
        "        Return: Tensor of shape num_samples x D.\n",
        "        \"\"\"\n",
        "        return self.decode(torch.randn((num_samples, self.d)).cuda())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf6OjxzFsr_H"
      },
      "source": [
        "### Training models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOgisYPGsr_I"
      },
      "outputs": [],
      "source": [
        "AE_d2 = AE(2, 28*28).cuda()\n",
        "optimizer = torch.optim.Adam(AE_d2.parameters(), lr=1e-3)\n",
        "\n",
        "train(AE_d2, optimizer, train_dl, test_dl, n_epochs=25)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldJipmAmsr_I"
      },
      "outputs": [],
      "source": [
        "AE_d10 = AE(10, 28*28).cuda()\n",
        "optimizer = torch.optim.Adam(AE_d10.parameters(), lr=1e-3)\n",
        "\n",
        "train(AE_d10, optimizer, train_dl, test_dl, n_epochs=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXJYy8d3sr_I"
      },
      "source": [
        "### Evaluating results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P3k6y8tsr_J"
      },
      "source": [
        "Visual inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtE6i4OBsr_J"
      },
      "outputs": [],
      "source": [
        "show_images(AE_d2.generate_samples(20).detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilYwqEYysr_J"
      },
      "outputs": [],
      "source": [
        "show_images(AE_d10.generate_samples(20).detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_like(model, data):\n",
        "    with torch.no_grad():\n",
        "        z = model.encode(data)\n",
        "        z_samples = torch.randn(64, z.shape[-1]).cuda() * 1.0 + z\n",
        "        alike_gen = model.decode(z_samples).squeeze().cpu().numpy().reshape(8, 8, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(alike_gen, (0, 2, 1, 3)).reshape(28*8, 28*8), cmap='gray')\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "CC8joKVroRCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_like(AE_d2, train_ds[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "id": "nCHAaQECouAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_like(AE_d10, train_ds[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "id": "fzwortSUpbRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xX4POHhpsr_J"
      },
      "source": [
        "Latent space visualization (from the decoder's side)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIUtad1Esr_K"
      },
      "outputs": [],
      "source": [
        "def draw_manifold_ae(model):\n",
        "    generator = lambda z: model.decode(torch.from_numpy(z).float().cuda()).view(28, 28).data.cpu().numpy()\n",
        "    return draw_manifold(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iU-RrUdLsr_K"
      },
      "outputs": [],
      "source": [
        "draw_manifold_ae(AE_d2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xgdt0q9dsr_L"
      },
      "source": [
        "Latent space visualization (from the encoder's side)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7vAjfDjsr_N"
      },
      "outputs": [],
      "source": [
        "draw_latent_space(AE_d2, test_ds[::10][0], test_ds[::10][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7YcFRSssr_O"
      },
      "outputs": [],
      "source": [
        "draw_latent_space(AE_d10, test_ds[::10][0], test_ds[::10][1], use_TSNE=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS8wtlzLsr_Q"
      },
      "source": [
        "## Probabilistic utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNZn-eTNsr_Q"
      },
      "outputs": [],
      "source": [
        "def bernoulli_log_likelihood(x_true, x_distr):\n",
        "    \"\"\"\n",
        "    Compute log-likelihood of objects x_true for the generated by model\n",
        "    component-wise Bernoulli distributions.\n",
        "    Each object from x_true has K corresponding distrbutions from x_distr.\n",
        "    Log-likelihood estimation must be computed for each pair of an object\n",
        "    and a corresponding to the object distribution.\n",
        "\n",
        "    Input: x_true, Tensor of shape n x D.\n",
        "    Input: x_distr, Tensor of shape n x K x D - parameters of component-wise\n",
        "           Bernoulli distributions.\n",
        "    Return: Tensor of shape n x K - log-likelihood for each pair of an object\n",
        "            and a corresponding distribution.\n",
        "    \"\"\"\n",
        "    x_true_ = x_true.unsqueeze(1)\n",
        "    return (x_true_*torch.log(x_distr) + (1-x_true_) * torch.log(1 - x_distr)).sum(dim=-1)\n",
        "\n",
        "\n",
        "def kl(q_mu, q_sigma, p_mu, p_sigma):\n",
        "    \"\"\"\n",
        "    \n",
        "    Compute KL-divergence KL(q || p) between n pairs of Gaussians\n",
        "    with diagonal covariational matrices.\n",
        "\n",
        "    Input: q_mu, p_mu, Tensor of shape n x d - mean vectors for n Gaussians.\n",
        "    Input: q_sigma, p_sigma, Tensor of shape n x d - standard deviation\n",
        "           vectors for n Gaussians.\n",
        "    Return: Tensor of shape n - each component is KL-divergence between\n",
        "            a corresponding pair of Gaussians.\n",
        "    \"\"\"\n",
        "    log_part = torch.log(p_sigma) - torch.log(q_sigma)\n",
        "    sqr_sum = (q_mu - p_mu)*(q_mu - p_mu) + q_sigma*q_sigma\n",
        "    div_part = 2*p_sigma*p_sigma\n",
        "    KL = log_part + sqr_sum/div_part - 0.5\n",
        "    return KL.sum(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9rtCgPNsr_R"
      },
      "source": [
        "## Variational Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJmX83Pdsr_R"
      },
      "outputs": [],
      "source": [
        "class ClampLayer(nn.Module):\n",
        "    def __init__(self, min=None, max=None):\n",
        "        super().__init__()\n",
        "        self.min = min\n",
        "        self.max = max\n",
        "\n",
        "    def forward(self, input):\n",
        "        return torch.clamp(input, self.min, self.max)\n",
        "\n",
        "\n",
        "class Reshape(torch.nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.reshape(x.shape[0], *self.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkRujG0Zsr_R"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "    def __init__(self, d, D):\n",
        "        \"\"\"\n",
        "        Initialize model weights.\n",
        "        Input: d, int - the dimensionality of the latent space.\n",
        "        Input: D, int - the dimensionality of the object space.\n",
        "        \"\"\"\n",
        "        super(type(self), self).__init__()\n",
        "        self.d = d\n",
        "        self.D = D\n",
        "        self.proposal_network = nn.Sequential(\n",
        "            nn.Linear(self.D, 400),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(400, 150),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(150, 80),\n",
        "            nn.LeakyReLU(),\n",
        "        )\n",
        "        self.proposal_mu_head = nn.Linear(80, self.d)\n",
        "        self.proposal_sigma_head = nn.Sequential(\n",
        "            nn.Linear(80, self.d),\n",
        "            nn.Softplus()\n",
        "        )\n",
        "        self.generative_network = nn.Sequential(\n",
        "            nn.Linear(self.d, 80),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(80, 150),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(150, 400),\n",
        "            nn.LeakyReLU(),\n",
        "\n",
        "            nn.Linear(400, self.D),\n",
        "            ClampLayer(-10, 10),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def proposal_distr(self, x):\n",
        "        \"\"\"\n",
        "        Generate proposal distribution over z.\n",
        "        Note that sigma is positive by design of neural network.\n",
        "        Input: x, Tensor of shape n x D.\n",
        "        Return: tuple(Tensor, Tensor),\n",
        "                Each Tensor is a matrix of shape n x d.\n",
        "        \"\"\"\n",
        "        mu = self.proposal_mu_head(self.proposal_network(x))\n",
        "        sigma = self.proposal_sigma_head(self.proposal_network(x))\n",
        "        return mu, sigma\n",
        "\n",
        "    def prior_distr(self, x):\n",
        "        \"\"\"\n",
        "        Generate prior distribution over z.\n",
        "\n",
        "        Input: x, Tensor of shape n x D.\n",
        "        Return: tuple(Tensor, Tensor),\n",
        "                Each Tensor is a matrix of shape n x d.\n",
        "        \"\"\"\n",
        "        n = x.size()[0]\n",
        "        mu = torch.zeros((n, self.d)).cuda()\n",
        "        sigma = torch.ones((n, self.d)).cuda()\n",
        "        return mu, sigma\n",
        "\n",
        "    def sample_latent(self, mu, sigma, K=1):\n",
        "        \"\"\"\n",
        "        Generate samples from Gaussians with diagonal covariance matrices in latent space.\n",
        "        Samples must be differentiable w. r. t. parameters of distribution!\n",
        "        Use reparametrization trick.\n",
        "        Input: mu, Tensor of shape n x d - mean vectors for n Gaussians.\n",
        "        Input: sigma, Tensor of shape n x d - standard deviation vectors\n",
        "               for n Gaussians.\n",
        "        Input: K, int - number of samples from each Gaussian.\n",
        "        Return: Tensor of shape n x K x d.\n",
        "        \"\"\"\n",
        "        n = mu.size()[0]\n",
        "        eps = torch.randn((n, K, self.d)).cuda()\n",
        "        return eps * sigma.unsqueeze(1) + mu.unsqueeze(1)\n",
        "\n",
        "    def generative_distr(self, z):\n",
        "        \"\"\"\n",
        "        Compute a tensor of parameters of Bernoulli distribution over x\n",
        "        given a tensor of latent representations.\n",
        "        Input: z, Tensor of shape n x K x d - tensor of latent representations.\n",
        "        Return: Tensor of shape n x K x D - parameters of Bernoulli distribution.\n",
        "        \"\"\"\n",
        "        n, K, _ = z.size()\n",
        "        return self.generative_network(z.view(-1, self.d)).view(n, K, self.D)\n",
        "\n",
        "    def calc_loss(self, batch):\n",
        "        \"\"\"\n",
        "        Compute VLB for batch. The VLB for batch is an average of VLBs for batch's objects.\n",
        "        VLB must be differentiable w. r. t. model parameters, so use reparametrization!\n",
        "        Input: batch, Tensor of shape n x D.\n",
        "        Return: Tensor, scalar - VLB.\n",
        "        \"\"\"\n",
        "        <YOUR_CODE>\n",
        "        return loss\n",
        "\n",
        "    def generate_samples(self, num_samples):\n",
        "        \"\"\"\n",
        "        Generate samples from the model.\n",
        "        Tip: for visual quality you may return the parameters of Bernoulli distribution instead\n",
        "        of samples from it.\n",
        "        Input: num_samples, int - number of samples to generate.\n",
        "        Return: Tensor of shape num_samples x D.\n",
        "        \"\"\"\n",
        "        mu, sigma = self.prior_distr(torch.zeros(size=(num_samples, )))\n",
        "        z = self.sample_latent(mu, sigma)\n",
        "        return self.generative_distr(z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GikVaBkisr_R"
      },
      "source": [
        "### Log-likelihood estimates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yHc2XJ_ssr_S"
      },
      "outputs": [],
      "source": [
        "def log_mean_exp(data):\n",
        "    max_ = torch.max(data, dim=-1).values.unsqueeze(-1)\n",
        "    data_exp = torch.exp(data - max_)\n",
        "    return torch.log(data_exp.mean(dim=-1)) + max_\n",
        "\n",
        "def compute_log_likelihood_monte_carlo(batch, model, generative_log_likelihood, K):\n",
        "    \"\"\"\n",
        "    Monte-Carlo log-likelihood estimation for a batch.\n",
        "\n",
        "    Input: batch, Tensor of shape n x D for VAE\n",
        "    Input: model, Module - object with methods prior_distr, sample_latent and generative_distr,\n",
        "           described in VAE class.\n",
        "    Input: generative_log_likelihood, function which takes batch and distribution parameters\n",
        "           produced by the generative network.\n",
        "    Input: K, int - number of latent samples.\n",
        "    Return: float - average log-likelihood estimate for the batch.\n",
        "    \"\"\"\n",
        "    mu, sigma = model.prior_distr(batch)\n",
        "    z = model.sample_latent(mu, sigma, K=K)\n",
        "    params = model.generative_distr(z)\n",
        "    log_likelihood = log_mean_exp(generative_log_likelihood(batch, params))\n",
        "    return log_likelihood.mean().data.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GwMgpJUsr_T"
      },
      "source": [
        "### Traning models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VAE_d2 = VAE(2, 28*28).cuda()\n",
        "optimizer = torch.optim.Adam(VAE_d2.parameters(), lr=7e-4)\n",
        "\n",
        "train(VAE_d2, optimizer, train_dl, test_dl, n_epochs=25, calc_LLMC=True)"
      ],
      "metadata": {
        "id": "BpznOAr-3xly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOqC88xJsr_T"
      },
      "outputs": [],
      "source": [
        "VAE_d10 = VAE(10, 28*28).cuda()\n",
        "optimizer = torch.optim.Adam(VAE_d10.parameters(), lr=7e-4)\n",
        "\n",
        "train(VAE_d10, optimizer, train_dl, test_dl, n_epochs=25, calc_LLMC=True, K=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2jjpmFCsr_T"
      },
      "source": [
        "### Evaluating results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0WuszFnsr_T"
      },
      "source": [
        "Visual inspection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVFV65a9sr_U"
      },
      "outputs": [],
      "source": [
        "show_images(VAE_d2.generate_samples(20).detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vQV7cnlsr_U"
      },
      "outputs": [],
      "source": [
        "show_images(VAE_d10.generate_samples(20).detach().cpu())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_like(model, data):\n",
        "    with torch.no_grad():\n",
        "        mu, sigma = model.proposal_distr(data)\n",
        "        z = model.sample_latent(mu, sigma, K=64)\n",
        "        alike_gen = model.generative_distr(z).squeeze().cpu().numpy().reshape(8, 8, 28, 28)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(np.transpose(alike_gen, (0, 2, 1, 3)).reshape(28*8, 28*8), cmap='gray')\n",
        "    plt.axis('off')"
      ],
      "metadata": {
        "id": "8kicd4Vtn_gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_like(VAE_d2, train_ds[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "id": "0ZkRFpRVoBZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_like(VAE_d10, train_ds[0][0].unsqueeze(0))"
      ],
      "metadata": {
        "id": "SDfGorGlldN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M04c3H1ysr_U"
      },
      "source": [
        "Latent space visualization (from the decoder's side)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKlBo9qBsr_U"
      },
      "outputs": [],
      "source": [
        "def draw_manifold_vae(model):\n",
        "    generator = lambda z: model.generative_distr(torch.from_numpy(z).unsqueeze(1).float().cuda()).view(28, 28).data.cpu().numpy()\n",
        "    return draw_manifold(generator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kk71eJQVsr_U"
      },
      "outputs": [],
      "source": [
        "draw_manifold_vae(VAE_d2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJCzur2msr_U"
      },
      "source": [
        "Latent space visualization (from the encoder's side)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def draw_latent_space_VAE(model, data, target, use_TSNE=False):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        z = model.sample_latent(*model.proposal_distr(data)).squeeze().cpu().numpy()\n",
        "    if (use_TSNE):\n",
        "        z = TSNE(2).fit_transform(z)\n",
        "\n",
        "    plt.figure(figsize=(7, 6))\n",
        "    plt.scatter(z[:, 0], z[:, 1], c=target.cpu().numpy(), cmap='gist_rainbow', alpha=0.75)\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "c_p4WqLJasqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3zb-5wPusr_U"
      },
      "outputs": [],
      "source": [
        "draw_latent_space_VAE(VAE_d2, test_ds[::10][0], test_ds[::10][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyEMhxAYsr_U"
      },
      "outputs": [],
      "source": [
        "draw_latent_space_VAE(VAE_d10, test_ds[::10][0], test_ds[::10][1], use_TSNE=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "N1zuTOYEbrbF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    },
    "colab": {
      "name": "lab_08_02_VAE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}