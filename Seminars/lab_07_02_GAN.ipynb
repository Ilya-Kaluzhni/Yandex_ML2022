{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/master/Seminars/lab_07_02_GAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKatwHToL7vN"
      },
      "source": [
        "# Generative adversarial networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY5JMnzgMb4P"
      },
      "source": [
        "Let's do our usual imports + use a tool to download the dataset we'll use today:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nereROxJhE4B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "from tqdm import tqdm\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYAIjRM1CLnq"
      },
      "source": [
        "The code below will download and preprocess the dataset we'll work with today:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP74Sd_tCD5R"
      },
      "outputs": [],
      "source": [
        "# Load Labeled Faces in the Wild dataset\n",
        "\n",
        "lfw = tfds.image_classification.LFW()\n",
        "lfw.download_and_prepare()\n",
        "ds = lfw.as_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMt5WWu_CFdo"
      },
      "outputs": [],
      "source": [
        "def get_img(x):\n",
        "    return x['image'][80:-80,80:-80]\n",
        "\n",
        "data = np.array([\n",
        "  np.array(Image.fromarray(img.numpy()).convert('L').resize((36, 36)))\n",
        "  for img in tqdm(ds['train'].map(get_img))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZr3EJGQCW9w"
      },
      "source": [
        "Let's have a look at the shape of the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDYZz-AwCU2R"
      },
      "outputs": [],
      "source": [
        "print(\"shape:\", data.shape)\n",
        "print(\"min, max:\", data.min(), data.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9S6ZuQaMntn"
      },
      "source": [
        "So far our data has the following shape: (n_images, height, width, n_channels). PyTorch convolutional layers want the channels dimension to be the second one (axis=1), so let's transpose (and normalize) the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZopvYBFmtSl"
      },
      "outputs": [],
      "source": [
        "data = data.astype(np.float32)[:, None, :, :] / 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "958mLZ1dNPZD"
      },
      "source": [
        "And here's a function to plot a (optionally random) subset of images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqtnOTerna5R"
      },
      "outputs": [],
      "source": [
        "def plot_mn(images, m=5, n=5, shuffle=True):\n",
        "    if shuffle:\n",
        "        images = images[np.random.permutation(len(images))[:m * n]]\n",
        "    h, w = images.shape[2:]\n",
        "    images = images[:m*n].reshape(m, n, *images.shape[1:]).transpose(0, 1, 3, 4, 2) # plotting requires channels last\n",
        "    images = images.transpose(0, 2, 1, 3, 4).reshape(m * h, n * w)\n",
        "    plt.imshow(images, cmap='gray')\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plot_mn(data)\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9Pg3gYBN05p"
      },
      "source": [
        "Finally, let's import torch and define the Reshape layer (same as in the convolutions notebook):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z7YAPGxp2Xl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Reshape(torch.nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.reshape(x.shape[0], *self.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORVT4PnCN_0h"
      },
      "source": [
        "Now, we'll take off from a very simple generator and discriminator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "33l9ywta2dyT"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"WARNING: gpu not found, the code will run on cpu\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f'Device is: \"{device}\".')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Db-LPTKOIhf"
      },
      "outputs": [],
      "source": [
        "latent_dims = 128\n",
        "batch_size = 64\n",
        "\n",
        "class Generator(torch.nn.Module):\n",
        "    def __init__(self, latent_dims):\n",
        "        super().__init__()\n",
        "        self.latent_dims = latent_dims\n",
        "        \n",
        "        self.generator = torch.nn.Sequential(\n",
        "            torch.nn.Linear(latent_dims, 64),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(64, 1 * 36 * 36),\n",
        "            Reshape(1, 36, 36),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "    \n",
        "    def forward(self, X):\n",
        "        return self.generator(X)\n",
        "\n",
        "\n",
        "class Discriminator(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.discriminator = torch.nn.Sequential(\n",
        "            Reshape(1 * 36 * 36),\n",
        "            torch.nn.Linear(1 * 36 * 36, 128),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(128, 1),\n",
        "        )\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.discriminator(X)\n",
        "\n",
        "class Sampler():\n",
        "    def __init__(self, data, batch_size, device):\n",
        "        self.data = torch.from_numpy(data).to(device)\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device\n",
        "        \n",
        "    def sample_true(self):\n",
        "        ids = np.random.choice(len(self.data), size=self.batch_size)\n",
        "        return self.data[ids]\n",
        "    \n",
        "    def sample_fake(self, G):\n",
        "        noise = torch.randn(self.batch_size, G.latent_dims).to(device)\n",
        "        return G(noise)\n",
        "        \n",
        "def get_n_params(model):\n",
        "    return sum(p.reshape(-1).shape[0] for p in model.parameters())\n",
        "\n",
        "G = Generator(latent_dims).to(device)\n",
        "D = Discriminator().to(device)\n",
        "sampler = Sampler(data, batch_size, device)\n",
        "\n",
        "print('generator params:', get_n_params(G))\n",
        "print('discriminator params:', get_n_params(D))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyYWtgV4Pn3G"
      },
      "source": [
        "Let's have a look what we can generate before any training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l_ju5AeyffR"
      },
      "outputs": [],
      "source": [
        "G.eval()\n",
        "\n",
        "imgs = sampler.sample_fake(G).cpu().detach().numpy()\n",
        "print(imgs.shape)\n",
        "plt.figure(figsize=(8, 8))\n",
        "plot_mn(imgs.clip(0, 1))\n",
        "plt.axis('off')\n",
        "plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLv6NgnbP2iO"
      },
      "source": [
        "Ok, now that we have our model defined, we need our loss functions. Historically the first loss used in GANs is the cross-entropy that we already used so many times:\n",
        "$$\\mathscr{L}^{\\text{discr}} =\n",
        "-\\text{E}_{x_{real} \\sim p(x)}\\left[logD_{\\phi}(x_{real})\\right]\n",
        "-\\text{E}_{z \\sim q(z)}\\left[log(1 - D_{\\phi}(G_{\\theta}(z)))\\right] \\rightarrow \\min_{\\phi}\n",
        "$$\n",
        "\n",
        "And hence for the generator the loss is:\n",
        "\n",
        "$$\n",
        "\\mathscr{L}^{\\text{gen}} =\n",
        "-\\text{E}_{z \\sim q(z)}\\left[logD_{\\phi}(G_{\\theta}(z)))\\right] \\rightarrow \\min_{\\theta}\n",
        "$$\n",
        "\n",
        "Note that here $D(x)$ is the probability the discriminator assigns to $x$ to be from the real dataset, so it's $\\sigma($ `discriminator` $(x))$.\n",
        "\n",
        "Try implementing these loss functions below. Note that $1-\\sigma(x)=\\sigma(-x)$. You should use the `logsigmoid` as a stable realization of $log\\cdot\\sigma(x)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVYlJGqlTJLJ"
      },
      "outputs": [],
      "source": [
        "logsigmoid = torch.nn.functional.logsigmoid\n",
        "\n",
        "def generator_loss(fake):\n",
        "    return <YOUR_CODE>\n",
        "  \n",
        "def discriminator_loss(real, fake):\n",
        "    return <YOUR_CODE>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YP1jCl68UGs_"
      },
      "source": [
        "Let's do some more set-up and run the learning process:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCLeJ9SQToLs"
      },
      "outputs": [],
      "source": [
        "generator_losses = []\n",
        "discriminator_losses = []\n",
        "\n",
        "generator_grad_norms = []\n",
        "discriminator_grad_norms = []\n",
        "\n",
        "optimizer_G = torch.optim.RMSprop(G.parameters(), lr=0.001)\n",
        "optimizer_D = torch.optim.RMSprop(D.parameters(), lr=0.001)\n",
        "\n",
        "scheduler_G = torch.optim.lr_scheduler.StepLR(optimizer_G, step_size=10, gamma=0.98)\n",
        "scheduler_D = torch.optim.lr_scheduler.StepLR(optimizer_D, step_size=10, gamma=0.98)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sf9fUuzz2dyW"
      },
      "outputs": [],
      "source": [
        "def calc_grad_norm(model):\n",
        "    total_norm = 0.0\n",
        "    for p in model.parameters():\n",
        "        param_norm = p.grad.data.norm(2)\n",
        "        total_norm += param_norm.item() ** 2\n",
        "    return np.sqrt(total_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRPr9l4tTqIN",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "for i in range(1200):\n",
        "    # Since our models are updated in turns,\n",
        "    # we first set the discriminator to train,\n",
        "    # while the generator is in the eval mode\n",
        "    G.eval()\n",
        "    D.train()\n",
        "  \n",
        "    # Several discriminator updates per step:\n",
        "    avg_D_grad_norm = 0.0\n",
        "    for j in range(5):\n",
        "        # Sampling reals and fakes\n",
        "        real = sampler.sample_true()\n",
        "        with torch.no_grad():\n",
        "            fake = sampler.sample_fake(G)\n",
        "\n",
        "        # Calculating the loss\n",
        "        loss = discriminator_loss(real, fake)\n",
        "\n",
        "        # Doing our regular optimization step for the discriminator\n",
        "        D.zero_grad()\n",
        "        loss.backward()\n",
        "        avg_D_grad_norm += calc_grad_norm(D)\n",
        "        optimizer_D.step()\n",
        "        \n",
        "    \n",
        "    # Remember the value of discriminator loss for plotting\n",
        "    avg_D_grad_norm /= 5\n",
        "    discriminator_losses.append(loss.item())\n",
        "    discriminator_grad_norms.append(avg_D_grad_norm)\n",
        "\n",
        "    # Now it's generator's time to learn:\n",
        "    G.train()\n",
        "    D.eval()\n",
        "\n",
        "    fake = sampler.sample_fake(G)\n",
        "    loss = generator_loss(fake)\n",
        "    \n",
        "    G.zero_grad()\n",
        "    loss.backward()\n",
        "    generator_grad_norms.append(calc_grad_norm(G))\n",
        "    optimizer_G.step()\n",
        "    \n",
        "    generator_losses.append(loss.item())\n",
        "\n",
        "    scheduler_D.step()\n",
        "    scheduler_G.step()\n",
        "\n",
        "    if i % 20 == 0:\n",
        "        G.eval()\n",
        "        imgs = sampler.sample_fake(G).cpu().detach().numpy()\n",
        "\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        plt.figure(figsize=(16, 8))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(generator_losses    , label='generator loss')\n",
        "        plt.plot(discriminator_losses, label='discriminator loss')\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plot_mn(imgs.clip(0, 1))\n",
        "        plt.axis('off')\n",
        "        \n",
        "        plt.show();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIKpwKUQ2dyX"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(generator_grad_norms, label='generator grad')\n",
        "plt.plot(discriminator_grad_norms, label='discriminator grad')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G.eval()\n",
        "num_images = 10\n",
        "Z = torch.linspace(-0.1, 0.2, num_images).unsqueeze(1).expand(num_images, latent_dims).cuda() # random (not random) noises\n",
        "\n",
        "plt.figure(figsize=(25, 10))\n",
        "for i in range(1, num_images + 1):\n",
        "    plt.subplot(1, num_images, i)\n",
        "    res = G(Z[i-1].unsqueeze(0)).squeeze().cpu().detach().numpy()\n",
        "    plt.imshow(res.clip(0, 1), cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "TJ26dsvs9vlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modifications"
      ],
      "metadata": {
        "id": "t-8Qds2Bm51R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Convolutional Neural Network (care about zero gradients)\n",
        "- Use ELU or LeakyReLU instead of ReLU\n",
        "- Use LayerNorm instead of BatchNorm2d\n",
        "- Decrease image size by convolutions or MeanPool2d instead of MaxPool2d\n",
        "- Use Transposed Convolutions instead of Upsampling2d (Difficult + Heavier)"
      ],
      "metadata": {
        "id": "2oM2RzY_n0Iq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Discriminator gradient L2 penalty for real images only (see this paper: https://arxiv.org/pdf/1801.04406.pdf):"
      ],
      "metadata": {
        "id": "kKL91C-BoCBO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUWQQK7_VRrR"
      },
      "source": [
        "```\n",
        "def discriminator_penalty(real):\n",
        "    real.requires_grad = True\n",
        "    scores = D(real)\n",
        "    grad = torch.autograd.grad(outputs=scores.mean(), inputs=real,\n",
        "        create_graph=True)[0]\n",
        "    penalty = (grad**2).sum()\n",
        "    return penalty\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Feature Matching Loss"
      ],
      "metadata": {
        "id": "Rzxhe3gbx3-p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "def feature_matching_loss(real, fake):\n",
        "    return ((D(real)[1] - D(fake)[1])**2).mean()\n",
        "```"
      ],
      "metadata": {
        "id": "lKBv-5kJx81n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus: Try replacing Upsample in Generator with nn.ConvTranspose2d()"
      ],
      "metadata": {
        "id": "EylJTJEH70iW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vk7oavpK7_9e"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab_07_02_GAN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}