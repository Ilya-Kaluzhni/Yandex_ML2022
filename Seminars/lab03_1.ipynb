{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/main/Seminars/lab03_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blQDweB5Z38c"
      },
      "source": [
        "# Linear classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4liWBRSDZ38d"
      },
      "source": [
        "We'll try to solve clients' churn task using data of mobile network operator.\n",
        "\n",
        "We have to predict whether customer will change the mobile network operator.\n",
        "\n",
        "The target field here is 'Churn'.\n",
        "\n",
        "Let's transform raw data, then make a Logistic Regression model and adjust it's parameteres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PL6XC3O8Z38e"
      },
      "source": [
        "Upload data and have a look at it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m23UWKQZ38f"
      },
      "outputs": [],
      "source": [
        "!wget -N https://raw.githubusercontent.com/yandexdataschool/MLatImperial2022/main/Data/telecom_churn2.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPRA-rbgZ38h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_state=0\n",
        "df = pd.read_csv('telecom_churn2.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4manWSUjZ38i"
      },
      "source": [
        "Transform target and  some other fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHLSE9PYZ38i"
      },
      "outputs": [],
      "source": [
        "d = {'Yes' : 1, 'No' : 0}\n",
        "df['International plan'] = df['International plan'].map(d)\n",
        "df['Voice mail plan'] = df['Voice mail plan'].map(d)\n",
        "df['Churn'] = df['Churn'].astype('int64')\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guH2q2ItZ38j"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "# find out how many missing values (numerical and categorical) are there.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHlaX47VZ38j"
      },
      "source": [
        "Divide data to design matrix X and target vector y.\n",
        "\n",
        "Make a train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kNOBMx5Z38k"
      },
      "outputs": [],
      "source": [
        "df.head()\n",
        "y=df['Churn']\n",
        "X=df.drop('Churn',axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iq_L0ApeZ38k"
      },
      "outputs": [],
      "source": [
        "X.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnkaT4FQZ38l"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "# analyse feature 'Area code' and transform it if nessesary\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlrBVzEhZ38m"
      },
      "source": [
        "Further we need to:\n",
        "- Impute missing numeric and categorical values.\n",
        "\n",
        "- Separate numerical and categorical fields.\n",
        "\n",
        "- Scale numerical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBuVVhINZ38m"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RZYtTyPZ38m"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_data = X_train.select_dtypes([np.number])\n",
        "numeric_data_mean = numeric_data.mean()\n",
        "X_train = X_train.fillna(numeric_data_mean)\n",
        "#or use inplace = True\n",
        "X_test = X_test.fillna(numeric_data_mean)\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O3pJm6MHZ38n"
      },
      "outputs": [],
      "source": [
        "numeric_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Vg49o17Z38o"
      },
      "source": [
        "Now we don't extract all the information from the data, simply because we do not use some of the features. These features in the dataset are encoded in strings, each of them represents a certain category. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1e800GwZ38o"
      },
      "source": [
        "Let's first fill in missing categorical features with special category \"NotGiven\". Sometimes the fact that a feature has a missing value can be a good sign itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3CVIT9mZ38p"
      },
      "outputs": [],
      "source": [
        "numeric_features = numeric_data.columns\n",
        "categorical = list(X_train.dtypes[X_train.dtypes == \"object\"].index)\n",
        "X_train[categorical] = X_train[categorical].fillna(\"NotGiven\")\n",
        "X_test[categorical] = X_test[categorical].fillna(\"NotGiven\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Z0arM2Z38p"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZn7xndwZ38q"
      },
      "source": [
        "### Categorical features encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRYwQjfeZ38q"
      },
      "source": [
        "Many ML algorithms do not work with categorial features and assume only numeric. If you want to transform categorial features into numeric, you may use encoding.  Two standard transformers from sklearn for working with categorical features are `OrdinalEncoder` (simply renumbers feature values with natural numbers) and `OneHotEncoder` (dummy features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wzczm0LZ38r"
      },
      "source": [
        "### One Hot Encoding\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lo-dlIiGZ38r"
      },
      "source": [
        "<img src=\"https://russianblogs.com/images/855/ddd65f4f342886bb411d41a33c5528e7.png\" width=50%> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuNbIUhHZ38s"
      },
      "source": [
        "A `OneHotEncoder` is a representation of categorical variables as binary vectors.\n",
        "\n",
        "`OneHotEncoder` assigns to each feature a whole vector consisting of zeros and one unit (which stands in the place corresponding to the received value, thus encoding it).\n",
        "\n",
        "Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOyI07AQZ38s"
      },
      "source": [
        "- Is it worth to apply a scaling transformer to features encoded by `OneHotEncoder`?\n",
        "- What's about applying  `OrdinalEncoder` in the case of a linear model? tree models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T43LXEkaZ38t"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJtp2HA1Z38t"
      },
      "source": [
        "We can write more streamlined  code with Pipeline:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/620/1*ONryJuHGGUZ6PUmYTMiFxQ.png\" width=50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtyAK57TZ38t"
      },
      "source": [
        "Model training is often presented as a sequence of some actions with training and test sets (for example, you first need to scale the sample (and for the training set you need to apply the fit method, and for the test set - transform), and then train/apply the model (for the train sample fit, and make predictions for test sample)  \n",
        "\n",
        "The `sklearn.pipeline.Pipeline` class allows you to store this sequence of steps and correctly applies it to both training and test samples.\n",
        "\n",
        "sklearn also has a class to make a pipeline without naming: `sklearn.pipeline.make_pipeline` \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DEWAz7bZ38u"
      },
      "source": [
        "### ColumnTransformer\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/537/1*BNwN3cmbLLoU9CQoJgFSKQ.png\" width=40%> \n",
        "\n",
        "\n",
        "We often need to apply different sets of tranformers to different groups of columns. For instance, we would want to apply OneHotEncoder to only categorical columns but not to numerical columns. This is where ColumnTransformer comes in. This time, we will partition the dataset keeping all columns so that we have both numerical and categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuhNwgqVZ38v"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV,LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\",sparse=False), categorical),\n",
        "    ('scaling', StandardScaler(), numeric_features)\n",
        "])\n",
        "X_train_encoded=column_transformer.fit_transform(X_train)\n",
        "\n",
        "pd.DataFrame(X_train_encoded).head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0EDXLiGDZ38w"
      },
      "outputs": [],
      "source": [
        "# Question:  does it nessecary to scale features for linear model?\n",
        "# what if you haven't got one-hot features?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXqq-4SbZ38x"
      },
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rA_g8IoZ38y"
      },
      "source": [
        "sklearn suggests 2 realizations of LogisticRegression:\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "\n",
        "- class sklearn.linear_model.LogisticRegression ()\n",
        "- class sklearn.linear_model.LogisticRegressionCV(*,\n",
        "\n",
        "                     Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2',\n",
        "                     \n",
        "                     scoring=None,  solver='lbfgs', tol=0.0001, max_iter=100,\n",
        "                     \n",
        "                     class_weight=None, n_jobs=None, verbose=0, refit=True, \n",
        "                     \n",
        "                     intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
        "                     \n",
        "   - Cs - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
        "   - penalty -{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’\n",
        "   - solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’c’. Algorithm to use in the optimization problem. Default is ‘lbfgs’. \n",
        "\n",
        "   - cv : cv  or cross-validation generator, default=5 folds\n",
        "            \n",
        "   - l1_ratios list of float, default=None. The list of Elastic-Net mixing parameter\n",
        "   \n",
        "In addition to the standard `fit`,`predict` methods, the `predict_proba()` method is useful for classifiers  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXxk-EnpZ38-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzTHBJ85Z38-"
      },
      "source": [
        "\n",
        "Let's make a logistic regression with L2-regularization in Pipeline with feature transformation, find the best parameters on cross-validation on the grid of the regularization parameter С: [0.0001,0.001,0.01,0.1,1,10,100].\n",
        "We'll use the LogisticRegressionCV and the number of cross-validation blocks cv=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47Iwq5yTZ38_"
      },
      "outputs": [],
      "source": [
        "#import warnings\n",
        "#warnings.simplefilter(\"ignore\")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression', LogisticRegressionCV(penalty='l2',Cs=[0.0001,0.001,0.01,0.1,1,10,100],max_iter=400,\n",
        "                                        cv=5,refit=True))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_proba = model.predict_proba(X_test)\n",
        "pd.DataFrame(y_proba[:, :]).head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6FzGpD_Z39A"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#increase max_iter parameter if solver can't converge and you see warnings 'STOP: TOTAL NO. of ITERATIONS REACHED LIMIT'\n",
        "# (or change solver parameter).\n",
        "#As the loss-function is convex, solver must converge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tS5yJtB_Z39A"
      },
      "outputs": [],
      "source": [
        "# Question: could feature scaling help if we see these warnings?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf2v4cZMZ39A"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#<calculate accuracy of model with warnings and compare it to accuracy of converged model without warnings  >"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQQvxeP6Z39B"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression', LogisticRegressionCV(penalty='l1',Cs=[0.1],solver='saga', max_iter=400,\n",
        "                                        cv=5,refit=True))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_proba = model.predict_proba(X_test)\n",
        "print(pd.DataFrame(y_proba[:, 1]).head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siOHn3GLZ39B"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('1. Test accuracy =' ,accuracy_score(y_pred,y_test))\n",
        "print(\"2. C = \" ,model['regression'].C_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ztZ98g6aZ39C"
      },
      "outputs": [],
      "source": [
        " #<YOUR TURN>\n",
        "# Try ElasticNet regularization instead of L1 and L2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHK1-5neZ39C"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#try to use here GridSearchCV and LogisticRegression instead of LogisticRegressionCV.\n",
        "#did you get the same accuracy result?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfH2wwHeZ39C"
      },
      "source": [
        "### Feature binarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TnU-ZtZZ39D"
      },
      "source": [
        "\n",
        "For feature binarization, you can use the class `sklearn.preprocessing.KBinsDiscretizer`:\n",
        "sklearn.preprocessing.KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', dtype=None)\n",
        "\n",
        "       strategy(default=’quantile’):\n",
        "            - uniform - \n",
        "            - quantile -  \n",
        "            - kmeans - 1D k-means cluster.\n",
        "\n",
        "Advantages of binarization: capturing non-monotonic and non-linear dependences feature from the target.\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuplE-4GZ39D"
      },
      "source": [
        "\n",
        "Instead of `StandardScaler`, we apply the class method `sklearn.preprocessing.KBinsDiscretizer` to numerical features with splitting into 15 groups and splitting strategy 'kmeans' to numerical features.\n",
        "At the same time we apply `OneHotEncoder` to categorical features.\n",
        "We use `ColumnTransformer` to combine uniformely these 2 transformation for the train and test datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GpYLR-pVZ39D"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PePfaE1sZ39E"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
        "    ('kbins',  KBinsDiscretizer(n_bins=20, strategy='uniform'), numeric_features)\n",
        "])\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression',  LogisticRegressionCV(penalty='l2',Cs=[0.0001,0.001,0.01,0.1,1,10,100],cv=5,max_iter=1000,\n",
        "                                       random_state=random_state))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test accuracy =\",accuracy_score(y_pred,y_test))\n",
        "print(\"C= \", model[1].C_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvPZ4MjzZ39E"
      },
      "source": [
        "#### Visualization of quantile binaization of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVUC95YpZ39F"
      },
      "outputs": [],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df['q_minutes'] = pd.qcut(df['Total intl minutes'], 11)\n",
        "df['service_calls'] = pd.cut(df['Customer service calls'], 5)\n",
        "df['share'] = pd.qcut(df['Total intl charge']/df['Total day charge'],11,precision=2)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6nUQis1Z39F"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.barplot(x= 'service_calls',y='Churn',data=df,color=\"blue\",saturation=0.25)\n",
        "plt.xlabel(\"'Total intl minutes'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqxqHmWZZ39G"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "sns.barplot(x= 'share',y='Churn',data=df,color=\"blue\",saturation=0.25)\n",
        "plt.xlabel(\"Total intl minutes\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCa-J0WRZ39H"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#make new variable  that makes sense and plot similar plot of it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVHW1msEZ39H"
      },
      "source": [
        "### Polynomial Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHk8G8ZhZ39I"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jn4nkBoxZ39I"
      },
      "outputs": [],
      "source": [
        "#you turn \n",
        "#apply polinomial features instead of Kbindiskretizer and calculate accuracy\n",
        "#compare time of running (use magic %%time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BlQ33IHZ39J"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
        "    ('binner',   PolynomialFeatures(2), numeric_features)\n",
        "])\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression',  LogisticRegressionCV(penalty='l2',Cs=[0.0001,0.001,0.01,0.1,1,10,100],cv=5,max_iter=1000,\n",
        "                                       random_state=random_state))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test accuracy = \", accuracy_score(y_pred,y_test))\n",
        "print(\"C= \",model[1].C_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlLUeZx_Z39J"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3k8-ETdZ39K"
      },
      "source": [
        "why polynomial features extracts more information from data than KBinsDiscretizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh-yIdAwZ39K"
      },
      "source": [
        "Is it worth to try polynomial featues of degree more than 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tWN7yTCZ39K"
      },
      "source": [
        "### GD  & SGD  for linear regresion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Up1ctLPhZ39L"
      },
      "source": [
        "$$L(\\beta | X, y) = \\| X\\beta - y \\|_2 \\to \\inf_{\\beta}$$\n",
        "\n",
        "$$\\beta = (X^TX)^{-1}X^Ty.$$\n",
        "\n",
        "Matrix inversion is a very time consuming operation that sometimes requires an unacceptable amount of resources $(O(d^3))$ and can be unstable.\n",
        "\n",
        "Therefore, parameters are often looked for using iterative methods. One of them is gradient descent.\n",
        "\n",
        "Recall that in the step of the gradient transition, the values of the parameters at the next step are obtained from the values of the parameters at the current step by shifting towards the antigradient of the functional:\n",
        "\n",
        "$$\\beta^{(t+1)} = \\beta^{(t)} - \\eta_t \\varepsilon \\nabla L(\\beta^{(t)}),$$\n",
        "where $\\eta_t \\varepsilon$ — step decrease dynamics.\n",
        "\n",
        "Formula for gradient in MSE case looks like:\n",
        "\n",
        "$$\\nabla L(\\beta) = -2X^Ty + 2X^TX\\beta = 2X^T(X\\beta - y).$$\n",
        " \n",
        "The complexity here is $O(dN)$. Stochastic gradient descent differs from basic gradient descent by replacing the gradient with an unbiased estimate for one or more objects. In this case, the complexity becomes $ O (kd) $, where $ k $ is the number of objects by which the gradient is estimated, $ k << N $. This partly explains the popularity of stochastic optimization techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BUc9fx2Z39L"
      },
      "source": [
        "### Vizualization of GD & SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkXepklBZ39L"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%pylab inline\n",
        "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mvw7G2IrZ39M"
      },
      "source": [
        "Let's generate a matrix of objects - features $ X $ and a vector of weights $ \\beta_ {true} $, calculate the vector of target numbers $ y $ as $ X\\beta_ {true} $ and add Gaussian noise:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fTZOdHkZ39M"
      },
      "outputs": [],
      "source": [
        "np.random.seed(16)\n",
        "n_features = 2\n",
        "n_objects = 300\n",
        "batch_size = 10\n",
        "num_steps = 43\n",
        "\n",
        "beta_true = np.random.normal(size=(n_features, ))\n",
        "\n",
        "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
        "X *= (np.arange(n_features) * 2 + 1)[None, :]  # for different scales\n",
        "Y = X.dot(beta_true) + np.random.normal(0, 1, (n_objects))\n",
        "beta_0 = np.random.uniform(-2, 2, (n_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqVBKFB5Z39N"
      },
      "source": [
        "Let us train linear regression for MSE on the obtained data using full gradient descent - thereby we obtain a vector of parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCLKEnDaZ39N"
      },
      "outputs": [],
      "source": [
        "beta = beta_0.copy()\n",
        "beta_list = [beta.copy()]\n",
        "step_size = 1e-2\n",
        "\n",
        "for i in range(num_steps):\n",
        "    beta -= 2 * step_size * np.dot(X.T, np.dot(X, beta) - Y) / Y.shape[0]\n",
        "    beta_list.append(beta.copy())\n",
        "beta_list = np.array(beta_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q89OOuzjZ39N"
      },
      "outputs": [],
      "source": [
        "#beta_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ML3Of1bZ39O"
      },
      "source": [
        "let's show a sequence of parameter estimates $\\beta^{(t)}$ obtained during iterations. The red dot is $\\beta_{true}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bjXQcyTZ39O"
      },
      "outputs": [],
      "source": [
        "# compute level set\n",
        "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "\n",
        "levels = np.empty_like(A)\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        beta_tmp = np.array([A[i, j], B[i, j]])\n",
        "        levels[i, j] = np.mean(np.power(np.dot(X, beta_tmp) - Y, 2))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('GD trajectory')\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_2$')\n",
        "plt.xlim((beta_list[:, 0].min() - 0.1, beta_list[:, 0].max() + 0.1))\n",
        "plt.ylim((beta_list[:, 1].min() - 0.1, beta_list[:, 1].max() + 0.1))\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "# visualize the level set\n",
        "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
        "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "\n",
        "# visualize trajectory\n",
        "plt.scatter(beta_true[0], beta_true[1], c='r')\n",
        "plt.scatter(beta_list[:, 0], beta_list[:, 1])\n",
        "plt.plot(beta_list[:, 0], beta_list[:, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYhJsGlbZ39O"
      },
      "source": [
        "We now visualize the trajectories of the stochastic gradient descent, repeating the same steps, while evaluating the gradient from the subsample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcCVCDLIZ39P"
      },
      "outputs": [],
      "source": [
        "beta = beta_0.copy()\n",
        "beta_list = [beta.copy()]\n",
        "step_size = 0.2\n",
        "\n",
        "for i in range(num_steps):\n",
        "    sample = np.random.randint(n_objects, size=batch_size)\n",
        "    beta -= 2 * step_size * np.dot(X[sample].T, np.dot(X[sample], beta) - Y[sample]) / Y.shape[0]\n",
        "    beta_list.append(beta.copy())\n",
        "beta_list = np.array(beta_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEneD6iIZ39P"
      },
      "outputs": [],
      "source": [
        "# compute level set\n",
        "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "\n",
        "levels = np.empty_like(A)\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        beta_tmp = np.array([A[i, j], B[i, j]])\n",
        "        levels[i, j] = np.mean(np.power(np.dot(X, beta_tmp) - Y, 2))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('SGD trajectory')\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_2$')\n",
        "plt.xlim((beta_list[:, 0].min() - 0.1, beta_list[:, 0].max() + 0.1))\n",
        "plt.ylim((beta_list[:, 1].min() - 0.1, beta_list[:, 1].max() + 0.1))\n",
        "plt.gca().set_aspect('equal')\n",
        "\n",
        "# visualize the level set\n",
        "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
        "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "\n",
        "# visualize trajectory\n",
        "plt.scatter(beta_true[0], beta_true[1], c='r')\n",
        "plt.scatter(beta_list[:, 0], beta_list[:, 1])\n",
        "plt.plot(beta_list[:, 0], beta_list[:, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7A_iIpwZ39P"
      },
      "source": [
        "As you can see, the stochastic gradient method \"wanders\" around the optimum. This is due to the selection of the step of the gradient descent $ \\eta_k $. The fact is that for the stochastic gradient descent to converge, the sequence of steps $ \\eta_k $ must satisfy the Robbins-Monroe conditions:\n",
        "$$\n",
        "\\sum_{k = 1}^\\infty \\eta_k = \\infty, \\qquad \\sum_{k = 1}^\\infty \\eta_k^2 < \\infty.\n",
        "$$\n",
        "Intuitively, this means the following:\n",
        "\n",
        "1. the sequence must diverge so that the optimization method can reach any point in space,\n",
        "2. but at the same time decrease quickly enough for the method to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeBhZrT6Z39Q"
      },
      "source": [
        "Let's try to look at the SGD trajectories, the sequence of steps satisfies the Robbins-Monroe conditions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sF7aRUEZ39Q"
      },
      "outputs": [],
      "source": [
        "beta = beta_0.copy()\n",
        "beta_list = [beta.copy()]\n",
        "step_size_0 = 0.45\n",
        "num_steps=100\n",
        "for i in range(num_steps):\n",
        "    step_size = step_size_0 / ((i+1)**0.6)\n",
        "    sample = np.random.randint(n_objects, size=batch_size)\n",
        "    beta -= 2 * step_size * np.dot(X[sample].T, np.dot(X[sample], beta) - Y[sample]) / Y.shape[0]\n",
        "    beta_list.append(beta.copy())\n",
        "beta_list = np.array(beta_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QDFIsDzZ39R"
      },
      "outputs": [],
      "source": [
        "# compute level set\n",
        "A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n",
        "\n",
        "levels = np.empty_like(A)\n",
        "for i in range(A.shape[0]):\n",
        "    for j in range(A.shape[1]):\n",
        "        beta_tmp = np.array([A[i, j], B[i, j]])\n",
        "        levels[i, j] = np.mean(np.power(np.dot(X, beta_tmp) - Y, 2))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.title('SGD trajectory')\n",
        "plt.xlabel(r'$\\beta_1$')\n",
        "plt.ylabel(r'$\\beta_2$')\n",
        "plt.xlim((beta_list[:, 0].min() - 0.1, beta_list[:, 0].max() + 0.1))\n",
        "plt.ylim((beta_list[:, 1].min() - 0.1, beta_list[:, 1].max() + 0.1))\n",
        "#plt.gca().set_aspect('equal')\n",
        "\n",
        "# visualize the level set\n",
        "CS = plt.contour(A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r)\n",
        "CB = plt.colorbar(CS, shrink=0.8, extend='both')\n",
        "\n",
        "# visualize trajectory\n",
        "plt.scatter(beta_true[0], beta_true[1], c='r')\n",
        "plt.scatter(beta_list[:, 0], beta_list[:, 1])\n",
        "plt.plot(beta_list[:, 0], beta_list[:, 1])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZ35aVoNZ39R"
      },
      "source": [
        "### Comparison of convergence rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUrUPrDNZ39S"
      },
      "outputs": [],
      "source": [
        "# data generation\n",
        "n_features = 50\n",
        "n_objects = 1000\n",
        "num_steps = 200\n",
        "batch_size = 2\n",
        "\n",
        "beta_true = np.random.uniform(-2, 2, n_features)\n",
        "\n",
        "X = np.random.uniform(-10, 10, (n_objects, n_features))\n",
        "Y = X.dot(beta_true) + np.random.normal(0, 5, n_objects)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHP-hUi8Z39S"
      },
      "outputs": [],
      "source": [
        "step_size_sgd = 1\n",
        "step_size_gd = 1e-2\n",
        "beta_sgd = np.random.uniform(-4, 4, n_features)\n",
        "beta_gd = beta_sgd.copy()\n",
        "residuals_sgd = [np.mean(np.power(np.dot(X, beta_sgd) - Y, 2))]\n",
        "residuals_gd = [np.mean(np.power(np.dot(X, beta_gd) - Y, 2))]\n",
        "\n",
        "for i in range(num_steps):\n",
        "    step_size = step_size_sgd / ((i+1) ** 0.51)\n",
        "    sample = np.random.randint(n_objects, size=batch_size)\n",
        "    beta_sgd -= 2 * step_size * np.dot(X[sample].T, np.dot(X[sample], beta_sgd) - Y[sample]) / Y.shape[0]\n",
        "    residuals_sgd.append(np.mean(np.power(np.dot(X, beta_sgd) - Y, 2)))\n",
        "    \n",
        "    beta_gd -= 2 * step_size_gd * np.dot(X.T, np.dot(X, beta_gd) - Y) / Y.shape[0]\n",
        "    residuals_gd.append(np.mean(np.power(np.dot(X, beta_gd) - Y, 2)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JypCEUL4Z39T"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 8))\n",
        "plt.plot(range(num_steps+1), residuals_gd, label='Basic Gradient Descent')\n",
        "plt.plot(range(num_steps+1), residuals_sgd, label='Stochastic Gradient Descent')\n",
        "plt.title('Empirial risk over iterations')\n",
        "plt.xlim((-1, num_steps+1))\n",
        "plt.legend()\n",
        "plt.xlabel('Iter num')\n",
        "plt.ylabel(r'Q($w$)')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQKrFQFnZ39T"
      },
      "source": [
        "### SGD Classifier in sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhhoHUaYZ39T"
      },
      "source": [
        "\n",
        "class sklearn.linear_model.SGDClassifier(loss='hinge', *, penalty='l2', alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, epsilon=0.1, n_jobs=None, random_state=None, learning_rate='optimal', eta0=0.0, power_t=0.5, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False, average=False)\n",
        "\n",
        "- loss , default=’hinge’. The loss function to be used. Defaults to ‘hinge’, which gives a linear SVM. The possible options are ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’, or a regression loss: ‘squared_error’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’.\n",
        "- penalty{‘l2’, ‘l1’, ‘elasticnet’}, default=’l2’\n",
        "- alpha , default=0.0001 regularization term\n",
        "- max_iter, default=1000 The maximum number of passes over the training data (aka epochs).\n",
        "- learning_rate , default=’optimal’:\n",
        "  -  ‘constant’: eta = eta0\n",
        "  - ‘optimal’: eta = 1.0 / (alpha * (t + t0)) where t0 is chosen by a heuristic proposed by Leon Bottou.\n",
        "  - ‘invscaling’: eta = eta0 / pow(t, power_t)\n",
        "  - ‘adaptive’: eta = eta0, as long as the training keeps decreasing. Each time n_iter_no_change consecutive epochs fail to decrease the training loss by tol or fail to increase validation score by tol if early_stopping is True, the current learning rate is divided by 5.\n",
        " - eta0 , default=0.0 The initial learning rate for the ‘constant’, ‘invscaling’ or ‘adaptive’ schedules. The default value is 0.0 as eta0 is not used by the default schedule ‘optimal’.\n",
        " \n",
        "#### The advantages of Stochastic Gradient Descent are:\n",
        "\n",
        "- Efficiency.\n",
        "\n",
        "- Ease of implementation (lots of opportunities for code tuning).\n",
        "\n",
        "#### the disadvantages of Stochastic Gradient Descent include:\n",
        "\n",
        "- SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
        "\n",
        "- SGD is sensitive to feature scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxqNoeCxZ39U"
      },
      "outputs": [],
      "source": [
        "results=[]\n",
        "for eps in [0.00001,0.0001,0.01,0.05,0.1,0.2,0.5,1.0]:\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('ohe_and_scaling', column_transformer),\n",
        "        ('regression', SGDClassifier(max_iter=100,loss='log',penalty='l2',alpha=0.1, \n",
        "                                     learning_rate='constant',eta0=eps,\n",
        "                                     random_state=random_state,n_iter_no_change=20))\n",
        "    ])\n",
        "\n",
        "    model = pipeline.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\" Test accuracy = %.4f learning rate= %.6f n_iter_=%.f\" % (accuracy_score(y_pred,y_test), eps,model[1].n_iter_))\n",
        "    results.append((accuracy_score(y_pred,y_test), eps))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPnqDrK8Z39U"
      },
      "outputs": [],
      "source": [
        "print(\"Max test accuracy = %.4f \\nlearning rate= %.4f\" % \n",
        "      (max(results, key = lambda i : i[0])[0],max(results, key = lambda i : i[0])[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDvQX9j6Z39U"
      },
      "source": [
        "Completely similar to the previous task, we will train the model with the learning_rate='adaptive' parameter (divides eps by 5 if there is no improvement in the training loss at several iterations . If you set too large eps, then it is very likely that it will not converge, it depends, in particular , from the n_iter_no_change parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ebISiwkwZ39V"
      },
      "outputs": [],
      "source": [
        "results=[]\n",
        "for eps in [1,5,10,100]:\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('ohe_and_scaling', column_transformer),\n",
        "        ('regression', SGDClassifier(max_iter=200,loss='log',penalty='l2',alpha=0.1,\n",
        "                                     learning_rate='adaptive',eta0=eps,\n",
        "                                     random_state=random_state,n_iter_no_change=5 ))\n",
        "    ])\n",
        "\n",
        "    model = pipeline.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(eps,accuracy_score(y_pred,y_test),model[1].n_iter_)\n",
        "    results.append((accuracy_score(y_pred,y_test), eps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1fwJSTbZ39V"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#try to change parameteres to get better results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4WfP7pZZ39V"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "lab03_1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}