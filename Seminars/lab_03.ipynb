{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oC9hZsXULwDE"
      },
      "source": [
        "  <a href=\"https://colab.research.google.com/github/ikitova/MLatImperial2022/main/blob/lab_02_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtA_MbD5LwDI"
      },
      "source": [
        "# Linear classification for real task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyXr0mesLwDJ"
      },
      "source": [
        "We'll try to solve clients' churn task using data of mobile network operator.\n",
        "\n",
        "We have to predict whether customer will change the mobile network operator.\n",
        "\n",
        "The target field here is 'Churn'.\n",
        "\n",
        "Let's transform raw data, then make a Logistic Regression model and adjust it's parameteres."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v9dEC5wLwDK"
      },
      "source": [
        "Upload data and have a look at it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YqjfWfqLwDL"
      },
      "outputs": [],
      "source": [
        "!wget -N https://raw.githubusercontent.com/yandexdataschool/MLatImperial2022/main/Data/telecom_churn2.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Av1rb8JoLwDM"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "random_state=0\n",
        "df = pd.read_csv('telecom_churn2.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuj8pRq0LwDP"
      },
      "source": [
        "Transform target and  some other fields"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7gAl8GHLwDQ"
      },
      "outputs": [],
      "source": [
        "d = {'Yes' : 1, 'No' : 0}\n",
        "df['International plan'] = df['International plan'].map(d)\n",
        "df['Voice mail plan'] = df['Voice mail plan'].map(d)\n",
        "df['Churn'] = df['Churn'].astype('int64')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_TexA54LwDR"
      },
      "source": [
        "Divide data to design matrix X and target vector y.\n",
        "\n",
        "Make a train-test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32ZXSjZMLwDR"
      },
      "outputs": [],
      "source": [
        "#df=df.drop('State',axis=1)\n",
        "df.head()\n",
        "y=df['Churn']\n",
        "X=df.drop('Churn',axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2,\n",
        "                                                    random_state=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXEz_9srLwDS"
      },
      "outputs": [],
      "source": [
        "X.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_pDI-VhPLwDS"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "# analyse feature 'Area code' and transform it if nessesary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5wCBYWBLwDT"
      },
      "source": [
        "Further we need to:\n",
        "- Impute missing numeric and categorical values.\n",
        "\n",
        "- Separate numerical and categorical fields.\n",
        "\n",
        "- Scale numerical features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ovp8cEZQLwDU"
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QayGQMFgLwDU"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "numeric_data = X_train.select_dtypes([np.number])\n",
        "numeric_data_mean = numeric_data.mean()\n",
        "X_train = X_train.fillna(numeric_data_mean)\n",
        "X_test = X_test.fillna(numeric_data_mean)\n",
        "X_train.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zoQjkZzLwDV"
      },
      "source": [
        "Now we don't extract all the information from the data, simply because we do not use some of the features. These features in the dataset are encoded in strings, each of them represents a certain category. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6meCfA9ULwDW"
      },
      "source": [
        "Let's first fill in missing categorical features with special category \"NotGiven\". Sometimes the fact that a feature has a missing value can be a good sign itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKotTpRlLwDW"
      },
      "outputs": [],
      "source": [
        "numeric_features = numeric_data.columns\n",
        "categorical = list(X_train.dtypes[X_train.dtypes == \"object\"].index)\n",
        "X_train[categorical] = X_train[categorical].fillna(\"NotGiven\")\n",
        "X_test[categorical] = X_test[categorical].fillna(\"NotGiven\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOUrbM53LwDX"
      },
      "outputs": [],
      "source": [
        "X_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw9aYrBfLwDY"
      },
      "source": [
        "### Categorical features encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS886mAHLwDY"
      },
      "source": [
        "Many ML algorithms do not work with categorial features and assume only numeric. If you want to transform categorial features into numeric, you may use encoding.  Two standard transformers from sklearn for working with categorical features are `OrdinalEncoder` (simply renumbers feature values with natural numbers) and `OneHotEncoder` (dummy features)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XkRMwamLwDZ"
      },
      "source": [
        "### One Hot Encoding\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWc4CSMLwDZ"
      },
      "source": [
        "<img src=\"https://russianblogs.com/images/855/ddd65f4f342886bb411d41a33c5528e7.png\" width=50%> \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu3Clzs3LwDZ"
      },
      "source": [
        "A `OneHotEncoder` is a representation of categorical variables as binary vectors.\n",
        "\n",
        "`OneHotEncoder` assigns to each feature a whole vector consisting of zeros and one unit (which stands in the place corresponding to the received value, thus encoding it).\n",
        "\n",
        "Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n",
        "\n",
        "Is it worth to apply a scaling transformer to features encoded by `OneHotEncoder`?\n",
        "What's about applying  `OrdinalEncoder` in the case of a linear model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5nagvrTLwDa"
      },
      "source": [
        "### Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNHsLIvpLwDa"
      },
      "source": [
        "We can write more streamlined  code with Pipeline:\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/620/1*ONryJuHGGUZ6PUmYTMiFxQ.png\" width=50%>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcA1LK8oLwDb"
      },
      "source": [
        "Model training is often presented as a sequence of some actions with training and test sets (for example, you first need to scale the sample (and for the training set you need to apply the fit method, and for the test set - transform), and then train/apply the model (for the train sample fit, and make predictions for test sample)  \n",
        "\n",
        "The `sklearn.pipeline.Pipeline` class allows you to store this sequence of steps and correctly applies it to both training and test samples.\n",
        "\n",
        "sklearn also has a class to make a pipeline without naming: `sklearn.pipeline.make_pipeline` \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xveaVXEXLwDc"
      },
      "source": [
        "### ColumnTransformer\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/537/1*BNwN3cmbLLoU9CQoJgFSKQ.png\" width=30%> \n",
        "\n",
        "\n",
        "We often need to apply different sets of tranformers to different groups of columns. For instance, we would want to apply OneHotEncoder to only categorical columns but not to numerical columns. This is where ColumnTransformer comes in. This time, we will partition the dataset keeping all columns so that we have both numerical and categorical features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RmA_cjYLwDc"
      },
      "outputs": [],
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegressionCV,LogisticRegression\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
        "\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\",sparse=False), categorical),\n",
        "    ('scaling', StandardScaler(), numeric_features)\n",
        "])\n",
        "X_train_encoded=column_transformer.fit_transform(X_train)\n",
        "\n",
        "pd.DataFrame(X_train_encoded).head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w2shlNeLwDd"
      },
      "source": [
        "### LogisticRegression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2RaAs0xLwDd"
      },
      "source": [
        "We've got 2 realizations of LogisticRegression:\n",
        "\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "\n",
        "- class sklearn.linear_model.LogisticRegression ()\n",
        "- class sklearn.linear_model.LogisticRegressionCV(*,\n",
        "\n",
        "                     Cs=10, fit_intercept=True, cv=None, dual=False, penalty='l2',\n",
        "                     \n",
        "                     scoring=None,  solver='lbfgs', tol=0.0001, max_iter=100,\n",
        "                     \n",
        "                     class_weight=None, n_jobs=None, verbose=0, refit=True, \n",
        "                     \n",
        "                     intercept_scaling=1.0, multi_class='auto', random_state=None, l1_ratios=None)\n",
        "                     \n",
        "   - Cs - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
        "   - penalty -{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’\n",
        "   - solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’. Algorithm to use in the optimization problem. Default is ‘lbfgs’. \n",
        "\n",
        "   - cv : cvint or cross-validation generator, default=None\n",
        "            The default cross-validation generator(for example,KFold or LeaveOneOut)  used is Stratified K-Folds. If an integer is provided, then it is the number of folds used. \n",
        "   - l1_ratioslist of float, default=None. The list of Elastic-Net mixing parameter\n",
        "   \n",
        "In addition to the standard `fit`,`predict` methods, the `predict_proba()` method is useful for classifiers  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ovir3x5LwDq"
      },
      "source": [
        "\n",
        "Let's create a logistic regression with L2-regularization in Pipeline with feature transformation, find the best parameters on cross-validation on the grid of the regularization parameter С: [0.0001,0.001,0.01,0.1,1,10,100].\n",
        "We'll use the LogisticRegressionCV and the number of cross-validation blocks cv=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVEa30nhLwDr"
      },
      "outputs": [],
      "source": [
        "#import warnings\n",
        "#warnings.simplefilter(\"ignore\")\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression', LogisticRegressionCV(penalty='l2',Cs=[0.0001,0.001,0.01,0.1,1,10,100],\n",
        "                                        cv=5,refit=True))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "y_proba = model.predict_proba(X_test)\n",
        "print(pd.DataFrame(y_proba[:, 1]).head())\n",
        "#print(pd.DataFrame(y_proba).head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6R0LL0bLwDr"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(\"1. Test accuracy = %.4f\" % accuracy_score(y_pred,y_test))\n",
        "print(\"2. C = %.4f\" % model['regression'].C_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4WdWtqoLwDs"
      },
      "outputs": [],
      "source": [
        "#<YOUR TURN>\n",
        "#try to use here GridSearchCV and LogisticRegression  instead of LogisticRegressionCV.\n",
        "#did you get the same accuracy result?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAaiFG02LwDs"
      },
      "source": [
        "#### Feature binarization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WN7auQ7aLwDt"
      },
      "source": [
        "\n",
        "For feature binarization, you can use the class `sklearn.preprocessing.KBinsDiscretizer`:\n",
        "sklearn.preprocessing.KBinsDiscretizer(n_bins=5, *, encode='onehot', strategy='quantile', dtype=None)\n",
        "\n",
        "       strategy(default=’quantile’):\n",
        "            - uniform - \n",
        "            - quantile -  \n",
        "            - kmeans - 1D k-means cluster.\n",
        "            - encode: 'ordinal'\n",
        "Advantages of binarization: capturing non-monotonic and non-linear dependences feature from the target.\n",
        "       "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcYcpaWeLwDt"
      },
      "source": [
        "\n",
        "Instead of `StandardScaler`, we apply the class method `sklearn.preprocessing.KBinsDiscretizer` to numerical features with splitting into 25 groups and splitting strategy 'kmeans' to numerical features.\n",
        "At the same time we apply `OneHotEncoder` to categorical features.\n",
        "We use `ColumnTransformer` to combine uniformely these 2 transformation for the train and test datasets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8RdHJ0ULwDt"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import KBinsDiscretizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn7A76eDLwDu"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
        "    ('binner',  KBinsDiscretizer(n_bins=25, strategy='quantile'), numeric_features)\n",
        "])\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression',  LogisticRegressionCV(penalty='l2',Cs=[0.0001,0.001,0.01,0.1,1,10,100],cv=5,max_iter=1000,\n",
        "                                       random_state=random_state))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test accuracy = %.4f\" % accuracy_score(y_pred,y_test))\n",
        "print(\"C= %.4f\" % model[1].C_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onmM2iCjLwDu"
      },
      "outputs": [],
      "source": [
        "#solver='liblinear'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_31LP1DiLwDv"
      },
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPamBHjCLwDv"
      },
      "outputs": [],
      "source": [
        "#you turn \n",
        "#apply polinomial features instead of Kbindiskretizer and calculate accuracy\n",
        "#compare time of running (use magic %%time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LQxFqyMLwDv"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "\n",
        "\n",
        "column_transformer = ColumnTransformer([\n",
        "    ('ohe', OneHotEncoder(handle_unknown=\"ignore\"), categorical),\n",
        "    ('binner',   PolynomialFeatures(2), numeric_features)\n",
        "])\n",
        "\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('ohe_and_scaling', column_transformer),\n",
        "    ('regression',  LogisticRegressionCV(penalty='l1',Cs=[0.0001,0.001,0.01,0.1,1,10,100],cv=5,solver='saga',max_iter=1000,\n",
        "                                       random_state=random_state))\n",
        "])\n",
        "\n",
        "model = pipeline.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Test accuracy = %.4f\" % accuracy_score(y_pred,y_test))\n",
        "print(\"C= %.4f\" % model[1].C_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMjfZfCSLwDx"
      },
      "source": [
        "Visualization of quantile binaization of features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eAQvrdgWLwDx"
      },
      "outputs": [],
      "source": [
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df['q_minutes'] = pd.qcut(df['Total intl minutes'], 11)\n",
        "df['service_calls'] = pd.cut(df['Customer service calls'], 5)\n",
        "df['share'] = pd.qcut(df['Total intl charge']/df['Total day charge'],11,precision=2)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P68WKoqzLwDy"
      },
      "outputs": [],
      "source": [
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "sns.barplot(x= 'service_calls',y='Churn',data=df,color=\"blue\",saturation=0.25)\n",
        "plt.xlabel(\"'Total intl minutes'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDuxLqi1LwDy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14,5))\n",
        "sns.barplot(x= 'share',y='Churn',data=df,color=\"blue\",saturation=0.25)\n",
        "plt.xlabel(\"Total intl minutes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZRb0UYALwDz"
      },
      "source": [
        "why polynomial features extracts more information from data than KBinsDiscretizer?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M33KR_y5LwDz"
      },
      "source": [
        "Is it worth to try polynomial featues of degree more than 2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXctJWpQLwD0"
      },
      "source": [
        "### Стохастический градиентый спуск в задачах классификации"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSW7q-GpLwD0"
      },
      "source": [
        "\n",
        "Решим задачу методом стохастического градиентого спуска,\n",
        "применяя такую же трансформацю признаков, как в последней задаче:\n",
        "    - KBinsDiscretizer с разбиением на 25 групп и стратегией разбиения 'kmeans' к численным признакам,\n",
        "    - OneHotEncoder- к категориальным.\n",
        "Воспользуемся классом моделей градиентного спуска\n",
        "`sklearn.linear_model.SGDClassifier` с параметрами:\n",
        "- learning_rate='constant'\n",
        "- max_iter=20 (сколько раз каждый объект случайно выбирается для модификации весов)\n",
        "- loss='log'\n",
        "- alpha=0.0001 (сила регуляризации)\n",
        "- penalty='l2'\n",
        "-  сетка значений шага скорости обучения epsilon (learning rate): [0.001,0.01,0.05,0.1,0.2,0.5,1.0,1.5,5,10] (Возьмем достаточно большие значения для иллюстрации расходимости)\n",
        "\n",
        "SGDRegressor(loss='squared_error', penalty='l2') решает ту же задачу, что и Ridge() (др.solver)\n",
        "\n",
        "Просто и быстро работает.\n",
        "\n",
        "Чувствителен к масштабу признаков, требует гиперпараметры, от которых может заметно зависеть качество (регуляризация, max_iter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQ_O1wJ_LwD1"
      },
      "outputs": [],
      "source": [
        "results=[]\n",
        "for eps in [0.001,0.01,0.05,0.1,0.2,0.5,1.0,1.5,5,10]:\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('ohe_and_scaling', column_transformer),\n",
        "        ('regression', SGDClassifier(max_iter=20,loss='log',penalty='l2',alpha=0.001, \n",
        "                                     learning_rate='constant',eta0=eps,\n",
        "                                     random_state=random_state,n_iter_no_change=20))\n",
        "    ])\n",
        "\n",
        "    model = pipeline.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\" Test accuracy = %.4f learning rate= %.4f\" % (accuracy_score(y_pred,y_test), eps))\n",
        "    results.append((accuracy_score(y_pred,y_test), eps))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3u-ma-aLwD1"
      },
      "source": [
        "Видим, как при  слишком большом learning rate SGD не сходится. Не достигает такого же качества как просто LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEOEsWPALwD2"
      },
      "outputs": [],
      "source": [
        "print(\"Max test accuracy = %.4f \\nlearning rate= %.4f\" % \n",
        "      (max(results, key = lambda i : i[0])[0],max(results, key = lambda i : i[0])[1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4Rsc41ALwD2"
      },
      "source": [
        "Полностью аналогично предыдущей задаче обучим модель с параметром learning_rate='adaptive'(делит eps на 5, если нет улучшения  training loss на нескольких итерациях(число задается другими параметрами). Если задать слишком большой eps, то очень возможно не справится, зависит, в частности, от параметра n_iter_no_change и др.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL8ytnqFLwD3"
      },
      "outputs": [],
      "source": [
        "results=[]\n",
        "for eps in [1,2,3,10]:\n",
        "    from sklearn.linear_model import SGDClassifier\n",
        "    pipeline = Pipeline(steps=[\n",
        "        ('ohe_and_scaling', column_transformer),\n",
        "        ('regression', SGDClassifier(max_iter=20,loss='log',penalty='l2',alpha=0.001,\n",
        "                                     learning_rate='adaptive',eta0=eps,\n",
        "                                     random_state=random_state,n_iter_no_change=5 ))\n",
        "    ])\n",
        "\n",
        "    model = pipeline.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(eps,accuracy_score(y_pred,y_test),model[1].n_iter_)\n",
        "    results.append((accuracy_score(y_pred,y_test), eps))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkNPcrR-LwD3"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_grid = {\n",
        "    'loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron'],\n",
        "    'penalty': ['elasticnet', 'l2', 'l1'],\n",
        "    'alpha': [10 ** x for x in range(-6, 1)],\n",
        "    'l1_ratio': [0, 0.05, 0.1, 0.2, 0.5, 0.8, 0.9, 0.95, 1],\n",
        "    'learning_rate' : [\"optimal\"], \n",
        "    'max_iter' : [100, 500, 1000, 1500, 2000]\n",
        "}\n",
        "\n",
        "sdgc = SGDClassifier()\n",
        "\n",
        "gs_sgdc = GridSearchCV(sdgc, param_grid=param_grid, cv=5, scoring=\"accuracy\", verbose=1)\n",
        "gs_sgdc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Msr94-neLwD4"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "lab_03.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}