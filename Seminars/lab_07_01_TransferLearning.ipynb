{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/master/Seminars/lab_07_01_TransferLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Vp1rWVq_V3iY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yphkUietvqk4"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9K3w4WhvvFK"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/.kaggle\n",
        "!cp /content/gdrive/My\\ Drive/kaggle.json /content/.kaggle/\n",
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "!ls -l /content/.kaggle\n",
        "\n",
        "%env KAGGLE_CONFIG_DIR=/content/.kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNOO05z7mioD"
      },
      "source": [
        "Go to https://www.kaggle.com/c/dogs-vs-cats and accept the rules to be able to get the data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle"
      ],
      "metadata": {
        "id": "Qe3gG4CA1k2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlakcXWpv4-V"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download --force -c dogs-vs-cats\n",
        "!unzip dogs-vs-cats.zip\n",
        "!unzip train.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "BPKy5K9K4QNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yh_vLM3SvjnY"
      },
      "source": [
        "### Using pre-trained model\n",
        "\n",
        "Today we're going to build and fine-tune CNN based on weights pre-trained on ImageNet: the largest image classification dataset as of now.\n",
        "More about imagenet: http://image-net.org/\n",
        "Setup: classify from a set of 1000 classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2nU6lBGvjnh"
      },
      "outputs": [],
      "source": [
        "import scipy as sp\n",
        "import scipy.misc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIcg3V6UvjoB"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "# class labels\n",
        "LABELS_URL = 'https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json'\n",
        "labels = list(requests.get(LABELS_URL).json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-eIRVpvjoJ"
      },
      "outputs": [],
      "source": [
        "print(f\"labels len: {len(labels)}\")\n",
        "print(labels[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms-IVrLfvjoU"
      },
      "source": [
        "### TorchVision\n",
        "PyTorch has several companion libraries, one of them being [torchvision](https://github.com/pytorch/vision/tree/master/) - it contains a number of popular vision datasets, preprocessing tools and most importantly, [pre-trained models](https://github.com/pytorch/vision/tree/master/torchvision/models).\n",
        "\n",
        "For now, we're going to use torch Inception-v3 module."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "ZXuARu8_vjoX"
      },
      "source": [
        "We're gonna use the inception-v3 network:\n",
        "![img](https://3811644265-files.gitbook.io/~/files/v0/b/gitbook-28427.appspot.com/o/assets%2F-LK1Q5wVABDXPa7Mueaw%2F-LWJ8IPgylwd7IEfGxE2%2F-LWJHJ9-6CrYABFt1F4T%2F1_rXcdL9OV5YKlYyks9XK-wA.png?alt=media&token=de746b98-05fe-47fd-afc1-6d3a6f0f2ca6)\n",
        "\n",
        "Let's first look at the code here: [url](https://github.com/pytorch/vision/blob/master/torchvision/models/inception.py).\n",
        "\n",
        "![img](https://habrastorage.org/files/449/171/7f8/4491717f88c34940b67947c1bc769bcd.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MA86FCdpvjoa"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.inception import inception_v3\n",
        "\n",
        "model = inception_v3(pretrained=True,      # load existing weights\n",
        "                     transform_input=False, # preprocess input image the same way as in training\n",
        "                     progress=True) # progress bar\n",
        "\n",
        "model.aux_logits = False # don't predict intermediate logits (yellow layers at the bottom)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the number of (scalar) parameters:\n",
        "n_parameters = 0\n",
        "for parameter in model.parameters():\n",
        "    n_parameters += parameter.reshape(-1).shape[0]\n",
        "\n",
        "print(n_parameters)"
      ],
      "metadata": {
        "id": "nmO-UCGeIgRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUXPwExUvjom"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from skimage.transform import resize"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOw-Jotlvjou"
      },
      "source": [
        "### Predict class probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDPaNJZeE0sJ"
      },
      "outputs": [],
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/d/de/Northern_Royal_Albatross_-_Kaikorua_-_New_Zealand_%2839039196692%29.jpg -O albatross.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6l5_04ycvjoy"
      },
      "outputs": [],
      "source": [
        "img = resize(plt.imread('albatross.jpg'), (299, 299))\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(img)\n",
        "plt.show()\n",
        "\n",
        "img = torch.FloatTensor(img.reshape([1, 299, 299, 3]).transpose([0,3,1,2]))\n",
        "\n",
        "probs = torch.nn.functional.softmax(model(img), dim=-1)\n",
        "\n",
        "probs = probs.data.numpy()\n",
        "\n",
        "top_ix = probs.ravel().argsort()[-1:-10:-1]\n",
        "print ('top-10 classes are: \\n [prob : class label]')\n",
        "for l in top_ix:\n",
        "    print ('%.4f :\\t%s' % (probs.ravel()[l], labels[l].split(',')[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AYyAgo6vjo9"
      },
      "source": [
        "### Having fun with pre-trained nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqRjQGKEvjo_"
      },
      "outputs": [],
      "source": [
        "!wget http://cdn.com.do/wp-content/uploads/2017/02/Donal-Trum-Derogar.jpeg -O img.jpg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CCDlJZJuvjpH"
      },
      "outputs": [],
      "source": [
        "img = resize(plt.imread('img.jpg')[:-100,200:-150], (299,299))\n",
        "plt.imshow(img)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "img = torch.FloatTensor(img.reshape([1, 299, 299, 3]).transpose([0,3,1,2]))\n",
        "\n",
        "probs = torch.nn.functional.softmax(model(img), dim=-1)\n",
        "\n",
        "probs = probs.data.numpy()\n",
        "\n",
        "top_ix = probs.ravel().argsort()[-1:-10:-1]\n",
        "print ('top-10 classes are: \\n [prob : class label]')\n",
        "for l in top_ix:\n",
        "    print ('%.4f :\\t%s' % (probs.ravel()[l], labels[l].split(',')[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P95r0oWTvjpN"
      },
      "source": [
        "# Grand-quest: Dogs Vs Cats\n",
        "* original competition\n",
        "* https://www.kaggle.com/c/dogs-vs-cats\n",
        "* 25k JPEG images of various size, 2 classes (guess what)\n",
        "\n",
        "### Your main objective\n",
        "* In this seminar your goal is to fine-tune a pre-trained model to distinguish between the two rivaling animals\n",
        "* The first step is to just reuse some network layer as features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVvOx5-LvjpY"
      },
      "source": [
        "### As before, we will use auxilary function you have seen on Monday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHIqL1MCvjpa"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class Logger:\n",
        "  def __init__(self):\n",
        "    self.train_loss_batch = []\n",
        "    self.train_loss_epoch = []\n",
        "    self.test_loss_batch = []\n",
        "    self.test_loss_epoch = []\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    self.epoch_counter = 0\n",
        "    \n",
        "    self.accuracy = []\n",
        "\n",
        "  def fill_train(self, loss):\n",
        "    self.train_loss_batch.append(loss)\n",
        "    self.train_batches_per_epoch += 1\n",
        "\n",
        "  def fill_test(self, loss):\n",
        "    self.test_loss_batch.append(loss)\n",
        "    self.test_batches_per_epoch += 1\n",
        "    \n",
        "  def fill_accuracy(self, y_true, y_pred):    \n",
        "    self.accuracy.append(accuracy_score(y_true, y_pred))\n",
        "\n",
        "  def finish_epoch(self):\n",
        "    self.train_loss_epoch.append(np.mean(\n",
        "        self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "    ))\n",
        "    self.test_loss_epoch.append(np.mean(\n",
        "        self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "    ))\n",
        "    self.train_batches_per_epoch = 0\n",
        "    self.test_batches_per_epoch = 0\n",
        "    \n",
        "    clear_output()\n",
        "  \n",
        "    print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8} \\t test_acc: {:.8}\".format(\n",
        "              self.epoch_counter,\n",
        "              self.train_loss_epoch[-1],\n",
        "              self.test_loss_epoch [-1],\n",
        "              self.accuracy[-1]\n",
        "          ))\n",
        "    \n",
        "    self.epoch_counter += 1\n",
        "\n",
        "    plt.figure(figsize=(18, 5))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(self.train_loss_batch, label='train loss')\n",
        "    plt.xlabel('# batch iteration')\n",
        "    plt.ylabel('loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "    plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "    plt.legend()\n",
        "    plt.xlabel('# epoch')\n",
        "    plt.ylabel('loss')\n",
        "    \n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(self.accuracy, label='test acc')\n",
        "    plt.xlabel('# epoch')\n",
        "    plt.ylabel('acc')\n",
        "    plt.legend()    \n",
        "    \n",
        "    plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptWexz6rvjpf"
      },
      "source": [
        "We also introduce new functions, they are very convinient in PyTorch, when you need to work with data, that does not fit in memory but can be easily downloaded in batches, for example, images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml0fCj3Uvjph"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset \n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkpczC3Uvjpm"
      },
      "outputs": [],
      "source": [
        "class PathDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class inherits from pytorch dataset.\n",
        "    It defines, how the data will be downloaded and preprocessed.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_paths, transform_X=None):\n",
        "        self.data_paths = data_paths\n",
        "        self.transform_X = transform_X\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = Image.open(self.data_paths[index])\n",
        "        if self.transform_X:\n",
        "            x = self.transform_X(x)\n",
        "        y = \"cat\" in self.data_paths[index]\n",
        "        return x, np.float32(y)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYzBrwAvvjps"
      },
      "outputs": [],
      "source": [
        "# Define path to folder with images\n",
        "train_paths = [\"./train/\" + name for name in os.listdir(\"train/\")]\n",
        "\n",
        "# Here I split val/train half and half\n",
        "val_paths = train_paths[:12500]\n",
        "train_paths= train_paths[12500:]\n",
        "\n",
        "len(val_paths), len(train_paths), np.sum([\"cat\" in path for path in val_paths]),\\\n",
        "                                  np.sum([\"cat\" in path for path in train_paths])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C3j3P_Hvjpy"
      },
      "source": [
        "Since we are going to use pretrained model we need **TO MAKE SURE** that we preprocess the data in the same way, it was done during training.\n",
        "\n",
        "In this case, we need to\n",
        "\n",
        "- Resize the image\n",
        "- Normalise it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PuZzFSTvjp0"
      },
      "outputs": [],
      "source": [
        "# ImageNet mean and std based on millions of images\n",
        "means = np.array((0.485, 0.456, 0.406))\n",
        "stds = np.array((0.229, 0.224, 0.225))\n",
        "\n",
        "transform_X = transforms.Compose([\n",
        "    transforms.Resize((299, 299)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(means, stds),\n",
        "])\n",
        "\n",
        "subset_of_train = 5000\n",
        "subset_of_val = 1000\n",
        "\n",
        "# Init train dataloader\n",
        "train_ds = PathDataset(train_paths[:subset_of_train], transform_X=transform_X)\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, \n",
        "                                              batch_size=256,\n",
        "                                              shuffle=True)\n",
        "\n",
        "# Init validation dataloader\n",
        "val_ds = PathDataset(val_paths[:subset_of_val], transform_X=transform_X)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, \n",
        "                                            batch_size=256,\n",
        "                                            shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D55v_EjNvjp4"
      },
      "source": [
        "# Task 1. Use standard sklearn to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Xccduovjp6"
      },
      "source": [
        "So now, we will use loaded above Inception model and get its output. Since we do not want to have classifcation as in ImageNet, we substitute the last layer with identity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRnlzWdlvjp7"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QvmUzzpvjp_"
      },
      "outputs": [],
      "source": [
        "# create layer that returns unchanged input\n",
        "class Identity(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pY58aX8_vjqE"
      },
      "outputs": [],
      "source": [
        "# visualises loop progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Extract outputs of InceptionNet on train dataset\n",
        "model.eval()\n",
        "model.fc = Identity()\n",
        "model.to(device)\n",
        "\n",
        "new_X_train, new_y_train = [], []\n",
        "for (X_batch, y_batch) in tqdm(train_dl):\n",
        "    with torch.no_grad():\n",
        "        new_X_train.extend(model(X_batch.to(device)).detach().cpu().numpy())\n",
        "        new_y_train.extend(y_batch.detach().cpu().numpy())\n",
        "\n",
        "new_X_train = np.array(new_X_train)\n",
        "new_y_train = np.array(new_y_train)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rh5Nj4cbvjqH"
      },
      "outputs": [],
      "source": [
        "# Extract outputs of InceptionNet on validation dataset\n",
        "new_X_val, new_y_val = [], []\n",
        "for (X_batch, y_batch) in tqdm(val_dl):\n",
        "    with torch.no_grad():\n",
        "        new_X_val.extend(model(X_batch.to(device)).detach().cpu().numpy())\n",
        "        new_y_val.extend(y_batch.detach().cpu().numpy())\n",
        "        \n",
        "new_X_val = np.array(new_X_val)\n",
        "new_y_val = np.array(new_y_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ihYUkp9vjqK"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logreg = LogisticRegression(max_iter=400)\n",
        "logreg.fit(new_X_train, new_y_train)\n",
        "\n",
        "print((logreg.predict(new_X_val) == new_y_val).mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2IzWz2MvjqV"
      },
      "source": [
        "# Task 2. Use our backbone model (Inception) to train Head Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0uDHuXCvjqW"
      },
      "source": [
        "In reality, when you want to apply some pretrained (large) neural network to YOUR problem you don't really have many samples to train on.\n",
        "\n",
        "Let's say we have 1024 samples for train and 256 samples for validation.\n",
        "\n",
        "Let's train HEAD network on some subset of your training data. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PathDataset(Dataset):\n",
        "    \"\"\"\n",
        "    This class inherits from pytorch dataset.\n",
        "    It defines, how the data will be downloaded and preprocessed.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_paths, transform_X=None):\n",
        "        self.data_paths = data_paths\n",
        "        self.transform_X = transform_X\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        x = Image.open(self.data_paths[index])\n",
        "        if self.transform_X:\n",
        "            x = self.transform_X(x)\n",
        "        y = \"cat\" in self.data_paths[index]\n",
        "        return x.to(device), torch.tensor(y).float().to(device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_paths)"
      ],
      "metadata": {
        "id": "xxz4UaUyidd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jub3JsZ6vjqX"
      },
      "outputs": [],
      "source": [
        "subset_of_train = 256\n",
        "subset_of_val = 256\n",
        "\n",
        "HEAD_train_ds = PathDataset(train_paths[:subset_of_train], transform_X=transform_X)\n",
        "val_ds = PathDataset(val_paths[:subset_of_val], transform_X=transform_X)\n",
        "\n",
        "HEAD_train_dl = torch.utils.data.DataLoader(HEAD_train_ds, \n",
        "                                              batch_size=128,\n",
        "                                              shuffle=True)\n",
        "val_dl = torch.utils.data.DataLoader(val_ds, \n",
        "                                            batch_size=128,\n",
        "                                            shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "img0zJLMvjqa"
      },
      "source": [
        "Now we define our new NN head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0fl0SlWvjqb"
      },
      "outputs": [],
      "source": [
        "class HeadNet(nn.Module):\n",
        "    def __init__(self, backbone):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.backbone = backbone\n",
        "        self.head = nn.Sequential(\n",
        "            torch.nn.Linear(2048, 16),\n",
        "            torch.nn.ELU(),\n",
        "            torch.nn.Linear(16, 1)\n",
        "        )\n",
        "\n",
        "    def cache_train(self, dl_train):\n",
        "        self.train_cache = []\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dl_train:\n",
        "                self.train_cache.append((self.backbone(batch_X), batch_y.view(-1, 1)))\n",
        "        \n",
        "\n",
        "    def cache_val(self, dl_val):\n",
        "        self.val_cache = []\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dl_val:\n",
        "                self.val_cache.append((self.backbone(batch_X), batch_y.view(-1, 1)))\n",
        "        \n",
        "    def reset(self):\n",
        "        self.train_cache_iter = iter(self.train_cache)\n",
        "        self.val_cache_iter = iter(self.val_cache)\n",
        "        \n",
        "    def cached_forward(self, mode='train'):\n",
        "        if (mode == 'train'):\n",
        "            X, y = next(self.train_cache_iter)\n",
        "\n",
        "        if (mode == 'val'):\n",
        "            X, y = next(self.val_cache_iter)\n",
        "\n",
        "        return self.head(X), y\n",
        "\n",
        "    def forward(self, X):\n",
        "        out = self.backbone(X)\n",
        "        out = self.head(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def train_head(model, optimizer, dl_train, dl_val, criterion, n_epochs):\n",
        "    logger = Logger()\n",
        "\n",
        "    model.cache_train(dl_train)\n",
        "    model.cache_val(dl_val)\n",
        "    \n",
        "    for i_epoch in range(n_epochs):\n",
        "        model.reset()\n",
        "        model.head.train()\n",
        "        for _ in range(len(dl_train)):\n",
        "            optimizer.zero_grad()\n",
        "            out, y = model.cached_forward(mode='train')\n",
        "            loss = criterion(out, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            logger.fill_train(loss.item())\n",
        "            \n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        model.head.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for _ in range(len(dl_val)):\n",
        "                out, y = model.cached_forward(mode='val')\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                y_pred.extend(out.squeeze().detach().cpu().numpy())\n",
        "                y_true.extend(y.cpu().numpy())\n",
        "                logger.fill_test(loss.item())\n",
        "        logger.fill_accuracy(np.array(y_true), np.array(y_pred) > 0.5)\n",
        "        logger.finish_epoch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72rjBXZevjqh"
      },
      "source": [
        "And train it as before we did before"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA4bgaOEvjqj"
      },
      "outputs": [],
      "source": [
        "head_net = HeadNet(model).to(device)\n",
        "\n",
        "for param in head_net.backbone.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "criterion = torch.nn.BCEWithLogitsLoss() # Binary Cross Entropy with log-sum-exp trick (subtracting maximum)\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(head_net.head.parameters(), lr=learning_rate)\n",
        "\n",
        "train_head(head_net, optimizer, HEAD_train_dl, val_dl, criterion, n_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_C0AfPzvjqq"
      },
      "source": [
        "Impressive right?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(head_net.state_dict(), \"trained_head.pth\")"
      ],
      "metadata": {
        "id": "1-sjHubcEM8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_kBhL3gvjqr"
      },
      "source": [
        "# Task 3. Use pretrained net to define new model (Transfer Learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_LC4Q411T68"
      },
      "source": [
        "OK, now, to get even better result, one can finetune the body network as well.\n",
        "This procedure is unstable and require very small learning rate and simple optimisation algo.\n",
        "Also, since the body is huge, we can only work with small batch size to fit in GPU."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "head_net = HeadNet(model).to(device)\n",
        "head_net.load_state_dict(torch.load('trained_head.pth'))"
      ],
      "metadata": {
        "id": "qAeiybP7JKJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_TL(model, optimizer, scheduler, dl_train, dl_val, criterion, n_epochs):\n",
        "    logger = Logger()\n",
        "\n",
        "    for i_epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in dl_train:\n",
        "            optimizer.zero_grad()\n",
        "            out = model(batch_X)\n",
        "            loss = criterion(out, batch_y.view(-1, 1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            logger.fill_train(loss.item())\n",
        "            \n",
        "        y_true = []\n",
        "        y_pred = []\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dl_val:\n",
        "                out = model(batch_X)\n",
        "                loss = criterion(out, batch_y.view(-1, 1))\n",
        "\n",
        "                y_pred.extend(out.squeeze().detach().cpu().numpy())\n",
        "                y_true.extend(batch_y.cpu().numpy())\n",
        "                logger.fill_test(loss.item())\n",
        "        logger.fill_accuracy(np.array(y_true), np.array(y_pred) > 0.5)\n",
        "        logger.finish_epoch()"
      ],
      "metadata": {
        "id": "ZO-He412G9M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCv6lBy1vjrO"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "subset_of_train = 1024\n",
        "\n",
        "train_ds = PathDataset(train_paths[:subset_of_train], transform_X=transform_X)\n",
        "\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, \n",
        "                                              batch_size=32,\n",
        "                                              shuffle=True)\n",
        "\n",
        "for param in head_net.backbone.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "\n",
        "loss_function = torch.nn.BCEWithLogitsLoss()\n",
        "learning_rate = 5e-4\n",
        "optimizer = torch.optim.SGD(head_net.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_TL(head_net, optimizer, scheduler, train_dl, val_dl, criterion, n_epochs=18)"
      ],
      "metadata": {
        "id": "Ds9K2cdmIV06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bonus reading: [Incremental learning](https://arxiv.org/pdf/1705.04228.pdf)"
      ],
      "metadata": {
        "id": "r_8kVlizFF1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WkVchaTeFGfC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab_07_01_TransferLearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}