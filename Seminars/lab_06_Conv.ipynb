{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yandexdataschool/MLatImperial2022/blob/master/Seminars/lab_06_Conv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFe9irgz6Iyi"
      },
      "source": [
        "### Recap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkASxl6K5CaA"
      },
      "source": [
        "Some imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jRTV00-OsGB"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import FashionMNIST\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XvCareYbeOa"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    print(\"WARNING: gpu not found, the code will run on cpu\")\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(f'Device is: \"{device}\".')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3WH_yAi5N4F"
      },
      "source": [
        "A utility class to monitor the training procedure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ4KpNKs45ir"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "class Logger:\n",
        "    def __init__(self):\n",
        "        self.train_loss_batch = []\n",
        "        self.train_loss_epoch = []\n",
        "        self.test_loss_batch = []\n",
        "        self.test_loss_epoch = []\n",
        "        self.train_batches_per_epoch = 0\n",
        "        self.test_batches_per_epoch = 0\n",
        "        self.epoch_counter = 0\n",
        "\n",
        "    def fill_train(self, loss):\n",
        "        self.train_loss_batch.append(loss)\n",
        "        self.train_batches_per_epoch += 1\n",
        "\n",
        "    def fill_test(self, loss):\n",
        "        self.test_loss_batch.append(loss)\n",
        "        self.test_batches_per_epoch += 1\n",
        "\n",
        "    def finish_epoch(self, make_plot=True):\n",
        "        self.train_loss_epoch.append(np.mean(\n",
        "            self.train_loss_batch[-self.train_batches_per_epoch:]\n",
        "        ))\n",
        "        self.test_loss_epoch.append(np.mean(\n",
        "            self.test_loss_batch[-self.test_batches_per_epoch:]\n",
        "        ))\n",
        "        self.train_batches_per_epoch = 0\n",
        "        self.test_batches_per_epoch = 0\n",
        "    \n",
        "        if make_plot:\n",
        "            clear_output()\n",
        "  \n",
        "        print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8}\".format(\n",
        "                  self.epoch_counter,\n",
        "                  self.train_loss_epoch[-1],\n",
        "                  self.test_loss_epoch [-1]\n",
        "              ))\n",
        "    \n",
        "        self.epoch_counter += 1\n",
        "\n",
        "        if make_plot:\n",
        "            plt.figure(figsize=(11, 5))\n",
        "\n",
        "            plt.subplot(1, 2, 1)\n",
        "            plt.plot(self.train_loss_batch, label='train loss')\n",
        "            plt.xlabel('# batch iteration')\n",
        "            plt.ylabel('loss')\n",
        "            plt.legend()\n",
        "\n",
        "            plt.subplot(1, 2, 2)\n",
        "            plt.plot(self.train_loss_epoch, label='average train loss')\n",
        "            plt.plot(self.test_loss_epoch , label='average test loss' )\n",
        "            plt.legend()\n",
        "            plt.xlabel('# epoch')\n",
        "            plt.ylabel('loss')\n",
        "            plt.show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o_l1fZj5S_8"
      },
      "source": [
        "Make dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQMRQtc948Xr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class FashionMNISTDataset(Dataset):\n",
        "    def __init__(self, X, y=None, device='cuda'):\n",
        "        self.device = device\n",
        "        self.X, self.y = self.preprocess_data(X, y)\n",
        "        \n",
        "    def preprocess_data(self, X, y):\n",
        "        X_preproc = torch.tensor(X / 255.,\n",
        "                                    dtype=torch.float).reshape(-1, 28 * 28).to(self.device)\n",
        "        \n",
        "        if (y is None):\n",
        "            return X_preproc, None\n",
        "        \n",
        "        return X_preproc, torch.tensor(y).to(self.device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if (self.y is None):\n",
        "            return self.X[idx]\n",
        "        \n",
        "        return self.X[idx], self.y[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsg8p7EH5c0G"
      },
      "source": [
        "Our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOG5I0_G40fM"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.m =  torch.nn.Linear(input_dim, output_dim)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.m(X)\n",
        "    \n",
        "def train(model, optimizer, scheduler, dl_train, dl_test, criterion, n_epochs):\n",
        "    logger = Logger()\n",
        "    \n",
        "    for i_epoch in range(n_epochs):\n",
        "        model.train()\n",
        "        for batch_X, batch_y in dl_train:\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            loss = criterion(model(batch_X), batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            logger.fill_train(loss.item())\n",
        "            \n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch_X, batch_y in dl_test:\n",
        "                loss = criterion(model(batch_X), batch_y)\n",
        "                logger.fill_test(loss.item())\n",
        "\n",
        "        logger.finish_epoch()\n",
        "        scheduler.step()\n",
        "        \n",
        "def predict(model, dl_test):\n",
        "    model.eval()\n",
        "    prediction = torch.zeros((len(dl_test.dataset), ), dtype=torch.long).cuda()\n",
        "    idx = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_X , _ in dl_test:\n",
        "            pred = model(batch_X).squeeze()\n",
        "            size = pred.shape[0]\n",
        "            prediction[idx:idx + size] = torch.argmax(pred, dim=1)\n",
        "            idx += size\n",
        "    \n",
        "    return prediction\n",
        "\n",
        "def accuracy_score(y_pred, y_test):\n",
        "    return (y_pred == y_test).sum()/len(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ikMsY05nX_"
      },
      "source": [
        "Getting the data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nVqelB915AVr"
      },
      "outputs": [],
      "source": [
        "# Getting the train and test parts of the dataset\n",
        "data_train = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=True)\n",
        "\n",
        "data_test = FashionMNIST(\"FashionMNIST/\",\n",
        "                          download=True,\n",
        "                          train=False)\n",
        "\n",
        "# In fact, it's already stored as torch tensor, but we'll need\n",
        "# to work with the numpy representation, so let's do the convertion:\n",
        "X_train = data_train.train_data.numpy()\n",
        "y_train = data_train.train_labels.numpy()\n",
        "\n",
        "X_test = data_test.test_data.numpy()\n",
        "y_test = data_test.test_labels.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PR9Dv501q19X"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "ds_train = FashionMNISTDataset(X_train, y_train)\n",
        "ds_test = FashionMNISTDataset(X_test, y_test)\n",
        "\n",
        "dl_train = DataLoader(ds_train, batch_size = BATCH_SIZE, shuffle=True)\n",
        "dl_test = DataLoader(ds_test, batch_size = BATCH_SIZE, shuffle=False)\n",
        "y_test = torch.tensor(y_test).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qVKqwxh6VyW"
      },
      "source": [
        "#### Yesterday's model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piQqJzsRq19Y"
      },
      "outputs": [],
      "source": [
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "# Defining the loss function:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_dim = 28 * 28 # number of pixels per image\n",
        "output_dim = 10 # number of classes\n",
        "model = Model(input_dim, output_dim).to(device)\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size = 2, gamma = 0.5)\n",
        "\n",
        "train(model, optimizer, scheduler, dl_train, dl_test, criterion, n_epochs = 15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyX6XsGYq19Y"
      },
      "source": [
        "Let's visualize our model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer.param_groups[0]['lr']"
      ],
      "metadata": {
        "id": "r-szVQpiEzBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr0 = 0.005\n",
        "lr1 = 0.0005\n",
        "num_updates = 7\n",
        "gamma = (lr1/lr0)**(1.0/num_updates) # lr1 = lr0 * (gamma)**(num_updates)\n",
        "gamma"
      ],
      "metadata": {
        "id": "gBuyQ3eXEIXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(predict(model, dl_test), y_test)"
      ],
      "metadata": {
        "id": "llrAhaPbu4Ge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz\n",
        "from torchviz import make_dot"
      ],
      "metadata": {
        "id": "EjUZnVN7u_8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualise"
      ],
      "metadata": {
        "id": "e421s9olvEQh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9MU6r8sq19Y"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(dl_train))\n",
        "y_pred = model(x)\n",
        "loss = criterion(y_pred, y)\n",
        "\n",
        "make_dot(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEtx0XvANxbY"
      },
      "source": [
        "### Convolutional layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkE_JQKUNzB-"
      },
      "source": [
        "<img src='https://cdn-images-1.medium.com/max/1600/0*iqNdZWyNeCr5tCkc.' alt='CNN animation'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPJvoTAtaW5u"
      },
      "source": [
        "One filter is applied to all the channels of the input image image, i.e.:\n",
        "$$\n",
        "\\mathrm{Conv}(x,y,o) = \\sum_{i=x - T}^{x + T} \\sum_{j=y - T}^{y + T} \\sum_{c=1}^C F_{o}(i - (x - T), j - (y - T), c) \\cdot I(i,j,c)\n",
        "$$\n",
        "\n",
        "\n",
        "the output value F in position x, y and output channel o will be calculated by the formula above\n",
        "- I is the input image of size $\\mathbb{R}^{H \\times W \\times C}$\n",
        "- F is the kernel of size $\\mathbb{R}^{K \\times K \\times C}, K = 2T + 1$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y43sj00oXgcZ"
      },
      "source": [
        "<font color='red'>Question:</font>What will be the output size of the:\n",
        "\n",
        "- picture of size 1x3x3, applied conv filter 3x3, stride=1, padding=0\n",
        "- picture of size 1x10x10, applied conv filter 3x3, stride=1, padding=0\n",
        "- picture of size 1x10x10, applied conv filter 3x3, stride=1, padding=1\n",
        "\n",
        "- picture of size 3x20x20, applied conv filter 3x3, stride=3, padding=0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cECqaRmrZTua"
      },
      "source": [
        "Eventually, this is the formula to calculate output size\n",
        "$$\n",
        "H_{out} = \\lfloor \\frac{H_{input} + 2P − K}{S}+1 \\rfloor \\\\\n",
        "W_{out} = \\lfloor \\frac{W_{input} + 2P − K}{S}+1 \\rfloor\n",
        "$$\n",
        "\n",
        "- $H_{input}, W_{input}$ are input image sizes\n",
        "- $H_{out}, W_{out}$ are output conv feature sizes\n",
        "- K is the kernel size\n",
        "- P is the padding\n",
        "- S is the stride"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24tBqv1IN33J"
      },
      "source": [
        "<img src='https://cdn-images-1.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg' alt='img'>\n",
        "(image taken from https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTKgl8r7EYrq"
      },
      "source": [
        "A general view of the most common convolutional architecture is shown above. The main idea is to gradually reduce the size of the image while increasing the number of channels. This is motivated by the following:\n",
        "\n",
        " - It's expensive (in terms of memory) to make a lot of channels for a large image, while smaller sized images allow us to do so. Intuitively, there's a trade-off between image size and number of channels.\n",
        " - We actually don't need that many channels at lower levels since there's not that many distinct low-level features for an image. Higher level features are more complex and require more filters (channels).\n",
        " - At the left side of the diagram (for low-level features) we care more about the positional information (e.g. \"is this stroke located near that one?\"), while at the right side (high-level features) we want to know what kind of an object we see, rather than where exactly we see it (e.g. \"looks like there's furry face somewhere in this picture - I might be looking at a cat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dD5Djx8CIleO"
      },
      "source": [
        "#### Getting a grip on convolutions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYsdbRX6NFAJ"
      },
      "source": [
        "Let's get an image of a flower:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://upload.wikimedia.org/wikipedia/commons/b/ba/Flower_jtca001.jpg -O flower.jpg"
      ],
      "metadata": {
        "id": "VeySzP-gPcap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-mkrfvnq19a"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = torch.from_numpy(np.array(Image.open(\"flower.jpg\").convert(\"L\").resize((500, 350)))).unsqueeze(0).unsqueeze(0)/255.0\n",
        "img.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0NJ1dPwq19b"
      },
      "source": [
        "Apply Sobel convolution"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Horizontal Sobel filter:\n",
        "```\n",
        "[[ -1., 0., 1.],\n",
        " [ -2., 0., 2.],\n",
        " [ -1., 0., 1.]]\n",
        "```\n",
        "Vertical Sobel filter:\n",
        "```\n",
        "[[ -1., -2., -1.],\n",
        " [  0.,  0.,   0.],\n",
        " [  1.,  2.,  1.]]\n",
        "```"
      ],
      "metadata": {
        "id": "hHRofQUYRcvm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dCg9PjgPq19b"
      },
      "outputs": [],
      "source": [
        "Sobel_filter_h = <YOUR_CODE> / 8.0 # normalized Sobel filter\n",
        "Sobel_filter_v = <YOUR_CODE> / 8.0 # normalized Sobel filter\n",
        "\n",
        "# use torch.nn.functional.conv2d\n",
        "res_h = <YOUR_CODE>\n",
        "res_v = <YOUR_CODE>\n",
        "\n",
        "res = torch.sqrt(res_h ** 2 + res_v ** 2).clip(0.0, 1.0) # compute gradient norm\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.imshow(res.numpy(), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl4vI5liPMVj"
      },
      "source": [
        "Once you're done try other kernels and see how they affect the image – what features do they highlight?\n",
        "\n",
        "\n",
        "What will happen if you apply a convolution twice? `n` times?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXGyy279ROrL"
      },
      "source": [
        "#### Building a CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elM80oLHN6xb"
      },
      "source": [
        "Convolutional layers in torch expect their input to be of 4-dimensional shape: $(B, C, H, W)$. Here $B$ is the number of images per batch, $C$ is the number of channels (e.g. 1 for a greyscale image, 3 for an RGB one, or number of filters from the previous convolutional layer). $H$ and $W$ are height and width in pixels.\n",
        "\n",
        "This means, at the beggining of our network we need to reshape our images from $(B, 784)$ to $(B, 1, 28, 28)$. In the end we'll want to reshape it back to 2 dimensions in order to apply a linear connection.\n",
        "\n",
        "For some reason torch doesn't have a reshaping layer, so we'll implement our own:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dIr4xVPN_LZ"
      },
      "outputs": [],
      "source": [
        "class Reshape(torch.nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super().__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x.reshape(x.shape[0], *self.shape)\n",
        "    \n",
        "def conv_out_size(size, conv_params, num_conv_blocks):\n",
        "    for c in range(num_conv_blocks):\n",
        "        size = size + 2 * conv_params['padding'] - conv_params['kernel_size'][0] + 1\n",
        "        size = size//2\n",
        "    return size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMgkQ_sI-z0I"
      },
      "source": [
        "Ok, now let's create and train a convolutional NN!\n",
        "\n",
        "Do keep in mind the model architecture from the picture above. I.e. we want to gradually reduce the size of the image while increasing the number of channels. We also want at least one fully connected layer at the end of the network.\n",
        "\n",
        "Use `torch.nn.Conv2d` for convolutions and `torch.nn.MaxPool2d` for max pooling.\n",
        "Also try `torch.nn.BatchNorm2d` and `torch.nn.Dropout` regularizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTdPa76a-9qP"
      },
      "outputs": [],
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, input_dim, conv_params, dropout_p, output_dim):\n",
        "        super().__init__()\n",
        "        out_size = conv_out_size(input_dim[-1], conv_params, 2)\n",
        "        self.m =  torch.nn.Sequential(\n",
        "            Reshape(*input_dim),\n",
        "            ...\n",
        "            <YOUR_CODE>\n",
        "            ...\n",
        "            torch.nn.Linear(?, 10)\n",
        "        ).to(device)\n",
        "        \n",
        "    def forward(self, X):\n",
        "        return self.m(X)\n",
        "\n",
        "# Defining the loss function:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_dim = (1, 28, 28)\n",
        "conv_params = {'kernel_size': <YOUR_CODE>, 'padding': <YOUR_CODE>}\n",
        "dropout_p = <YOUR_CODE>\n",
        "output_dim = 10 # number of classes\n",
        "\n",
        "model = Model(input_dim, conv_params, dropout_p, output_dim).to(device)\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size = 2, gamma = 0.5)\n",
        "\n",
        "train(model, optimizer, scheduler, dl_train, dl_test, criterion, n_epochs = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-UBVPzbq19d"
      },
      "outputs": [],
      "source": [
        "accuracy_score(predict(model, dl_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Piev01Hq19d"
      },
      "source": [
        "Let's see our model graph now:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sflFqVCHq19d"
      },
      "outputs": [],
      "source": [
        "x, y = next(iter(dl_train))\n",
        "y_pred = model(x)\n",
        "loss = criterion(y_pred, y)\n",
        "\n",
        "make_dot(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRr6WeBMq19d"
      },
      "outputs": [],
      "source": [
        "def visualise_conv(f_maps):\n",
        "    N = f_maps.shape[0] # suppose N = k^2, k > 1\n",
        "    k = int(np.sqrt(N))\n",
        "    \n",
        "    fig, ax = plt.subplots(k, k, figsize=(10, 10))\n",
        "    for i in range(k):\n",
        "        for j in range(k):\n",
        "            im = ax[i, j].imshow(f_maps[i*k + j].cpu().numpy())\n",
        "            ax[i, j].axis('off')\n",
        "            \n",
        "    fig.subplots_adjust(right=0.8)\n",
        "    cbar_ax = fig.add_axes([0.85, 0.15, 0.05, 0.7])\n",
        "    fig.colorbar(im, cax=cbar_ax)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhCppc0Kq19e"
      },
      "outputs": [],
      "source": [
        "# Visualise learned kernels\n",
        "\n",
        "visualise_conv(model.m[1].weight.detach().squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6-cmkyiq19e"
      },
      "outputs": [],
      "source": [
        "activation = {}\n",
        "\n",
        "def get_activation(name):\n",
        "    def hook(model, input, output):\n",
        "        activation[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "model.m[1].register_forward_hook(get_activation('conv1'))\n",
        "model.m[5].register_forward_hook(get_activation('conv2'))\n",
        "\n",
        "x, _ = next(iter(dl_train))\n",
        "output = model(x[0].unsqueeze(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6r7UUJTq19e"
      },
      "outputs": [],
      "source": [
        "activation['conv1'].shape, activation['conv2'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3czMzCxq19e"
      },
      "outputs": [],
      "source": [
        "# Visualise conv feature maps\n",
        "\n",
        "visualise_conv(activation['conv1'].squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgTOZaLLq19f"
      },
      "outputs": [],
      "source": [
        "visualise_conv(activation['conv2'].squeeze())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7aVGl5gOPQL"
      },
      "outputs": [],
      "source": [
        "predictions_test = predict(model, dl_test).cpu().numpy()\n",
        "mask = predictions_test != y_test.cpu().numpy()\n",
        "\n",
        "wrongly_predicted_objects = X_test[mask]\n",
        "wrongly_predicted_labels = predictions_test[mask]\n",
        "wrongly_predicted_labels_true = y_test.cpu().numpy()[mask]\n",
        "\n",
        "label_names = np.array([\n",
        "    'T-shirt/top',\n",
        "    'Trouser',\n",
        "    'Pullover',\n",
        "    'Dress',\n",
        "    'Coat',\n",
        "    'Sandal',\n",
        "    'Shirt',\n",
        "    'Sneaker',\n",
        "    'Bag',\n",
        "    'Ankle boot',\n",
        "])\n",
        "\n",
        "\n",
        "# Print and plot the first 100:\n",
        "wrongly_predicted_labels = label_names[wrongly_predicted_labels[:100]].reshape(10, 10)\n",
        "wrongly_predicted_labels_true = label_names[wrongly_predicted_labels_true[:100]].reshape(10, 10)\n",
        "\n",
        "for ix in range(10):\n",
        "    for iy in range(10):\n",
        "        plt.text(ix / 5, iy / 8, (wrongly_predicted_labels[-1 - iy, ix]), color='red')\n",
        "        plt.text(ix / 5, iy / 8 + 0.05, (wrongly_predicted_labels_true[-1 - iy, ix]), color='green')\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.imshow(wrongly_predicted_objects[:100].reshape(10, 10, 28, 28)\n",
        "           .transpose(0, 2, 1, 3).reshape(280, 280), cmap=\"Greys\")\n",
        "plt.axis('off');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R4Zh6Ktq19f"
      },
      "source": [
        "## Bonus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tjejl3Hgq19f"
      },
      "source": [
        "Let's do some augmentation with [torchvision](https://pytorch.org/vision/stable/transforms.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFlqrPrPq19f"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjPPnqaQq19f"
      },
      "outputs": [],
      "source": [
        "X_train = data_train.train_data.numpy()\n",
        "y_train = data_train.train_labels.numpy()\n",
        "\n",
        "X_test = data_test.test_data.numpy()\n",
        "y_test = data_test.test_labels.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformation example"
      ],
      "metadata": {
        "id": "umwe-vrqbE7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "im = torch.from_numpy(X_test[0]).unsqueeze(0)/255.0\n",
        "trans = transforms.RandomRotation(degrees=(-50, 50))\n",
        "im_trans = trans(im).squeeze().numpy()\n",
        "\n",
        "plt.imshow(im_trans);"
      ],
      "metadata": {
        "id": "Y7GRtPnfaLXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNMDemMFq19g"
      },
      "outputs": [],
      "source": [
        "class FashionMNISTDataset(Dataset):\n",
        "    def __init__(self, X, y=None, device='cuda', do_aug=False):\n",
        "        self.device = device\n",
        "        self.X, self.y = self.preprocess_data(X, y)\n",
        "        self.do_aug = do_aug\n",
        "        \n",
        "        self.transforms = torch.nn.Sequential(\n",
        "            transforms.RandomRotation(degrees=(-10, 10)),\n",
        "            transforms.RandomPerspective(distortion_scale=0.2, p=0.5)\n",
        "        )\n",
        "        \n",
        "    def preprocess_data(self, X, y):\n",
        "        X_preproc = torch.tensor(X / 255.,\n",
        "                                    dtype=torch.float).reshape(-1, 28 * 28).to(self.device)\n",
        "        \n",
        "        X_preproc = (X_preproc - X_preproc.mean(1).unsqueeze(1))/X_preproc.std(1).unsqueeze(1)\n",
        "        if (y is None):\n",
        "            return X_preproc, None\n",
        "        \n",
        "        return X_preproc.reshape(-1, 28, 28), torch.tensor(y).to(self.device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.X.shape[0]\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        if (self.y is None):\n",
        "            return self.X[idx]\n",
        "\n",
        "        if (self.do_aug):\n",
        "            X_aug = self.transforms(self.X[idx].unsqueeze(0)).squeeze()\n",
        "        else:\n",
        "            X_aug = self.X[idx]\n",
        "\n",
        "        return X_aug.reshape(-1), self.y[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tFI8yOCmq19g"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "ds_train = FashionMNISTDataset(X_train, y_train, do_aug=True)\n",
        "ds_test = FashionMNISTDataset(X_test, y_test)\n",
        "\n",
        "dl_train = DataLoader(ds_train, batch_size = BATCH_SIZE, shuffle=True)\n",
        "dl_test = DataLoader(ds_test, batch_size = BATCH_SIZE, shuffle=False)\n",
        "y_test = torch.tensor(y_test).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6gCYY39q19g"
      },
      "outputs": [],
      "source": [
        "# Defining the loss function:\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Defining the model\n",
        "input_dim = (1, 28, 28)\n",
        "conv_params = {'kernel_size': (3, 3), 'padding': 1}\n",
        "dropout_p = 0.2\n",
        "output_dim = 10 # number of classes\n",
        "\n",
        "model = Model(input_dim, conv_params, dropout_p, output_dim).to(device)\n",
        "\n",
        "# Setting up the optimizer\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size = 2, gamma = 0.8)\n",
        "\n",
        "train(model, optimizer, scheduler, dl_train, dl_test, criterion, n_epochs = 15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9vIQpcNq19g"
      },
      "outputs": [],
      "source": [
        " accuracy_score(predict(model, dl_test), y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Tomorrow's lecture: Style Transfer\n",
        "- [Style Transfer in pytorch](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q91ijEJneayU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Shv6H2Bbesf8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "lab_06_Conv.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}